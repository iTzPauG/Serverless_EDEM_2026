{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c397ee4",
   "metadata": {},
   "source": [
    "## **Apache Beam Basics**\n",
    "\n",
    "Description: This notebook will teach you the basics of the Apache Beam Programming Model:\n",
    "\n",
    "- Apache Beam Basics: Pipeline, PCollection, Transforms & PTransforms.\n",
    "   - GroupByKey\n",
    "   - CoGroupByKey\n",
    "   - Flatten\n",
    "   - Partition.\n",
    "- Introducing complexity: DoFn.\n",
    "   - DoFn vs Map\n",
    "   - DoFn LifeCycle\n",
    "   - DoFn Stateful Processing\n",
    "- Advanced: Streaming.\n",
    "   - Fixed Windows\n",
    "   - Sliding Windows\n",
    "   - PubSub to Bigquery\n",
    "\n",
    "\n",
    "EDEM. Master Big Data & Cloud 2025/2026<br>\n",
    "Professor: Javi Briones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d25925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"apache-beam[gcp]\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8a49fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apache-beam==2.62.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2.62.0)\n",
      "Requirement already satisfied: google-cloud-firestore==2.23.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from -r ../requirements.txt (line 2)) (2.23.0)\n",
      "Requirement already satisfied: pandas==2.3.3 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from -r ../requirements.txt (line 3)) (2.3.3)\n",
      "Requirement already satisfied: soundfile==0.13.1 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from -r ../requirements.txt (line 4)) (0.13.1)\n",
      "Requirement already satisfied: tensorflow==2.18.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from -r ../requirements.txt (line 5)) (2.18.0)\n",
      "Requirement already satisfied: crcmod==1.7 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from -r ../requirements.txt (line 6)) (1.7)\n",
      "Requirement already satisfied: torch==2.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from -r ../requirements.txt (line 7)) (2.6.0)\n",
      "Requirement already satisfied: torchaudio==2.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from -r ../requirements.txt (line 8)) (2.6.0)\n",
      "Requirement already satisfied: transformers==4.45.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from -r ../requirements.txt (line 9)) (4.45.0)\n",
      "Requirement already satisfied: orjson<4,>=3.9.7 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (3.11.5)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (0.3.1.1)\n",
      "Requirement already satisfied: cloudpickle~=2.2.1 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: fastavro<2,>=0.23.6 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (1.12.1)\n",
      "Requirement already satisfied: fasteners<1.0,>=0.3 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (0.20)\n",
      "Requirement already satisfied: grpcio!=1.48.0,!=1.59.*,!=1.60.*,!=1.61.*,!=1.62.0,!=1.62.1,<1.66.0,<2,>=1.33.1 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (1.65.5)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2.7.3)\n",
      "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (0.22.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (4.26.0)\n",
      "Requirement already satisfied: jsonpickle<4.0.0,>=3.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: numpy<2.3.0,>=1.14.3 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2.0.2)\n",
      "Requirement already satisfied: objsize<0.8.0,>=0.6.1 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (0.7.1)\n",
      "Requirement already satisfied: packaging>=22.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (4.16.0)\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (1.27.0)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<6.0.0.dev0,>=3.20.3 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (5.29.5)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: redis<6,>=5.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (5.3.1)\n",
      "Requirement already satisfied: regex>=2020.6.8 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2026.1.15)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: zstandard<1,>=0.18.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (0.25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=3.12 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (6.0.3)\n",
      "Requirement already satisfied: pyarrow<17.0.0,>=3.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix<1 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (0.7)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-cloud-firestore==2.23.0->-r ../requirements.txt (line 2)) (2.29.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-cloud-firestore==2.23.0->-r ../requirements.txt (line 2)) (2.48.0rc0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0,>=1.4.1 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-cloud-firestore==2.23.0->-r ../requirements.txt (line 2)) (2.5.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from pandas==2.3.3->-r ../requirements.txt (line 3)) (2025.3)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from soundfile==0.13.1->-r ../requirements.txt (line 4)) (2.0.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from tensorflow==2.18.0->-r ../requirements.txt (line 5)) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from tensorflow==2.18.0->-r ../requirements.txt (line 5)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from tensorflow==2.18.0->-r ../requirements.txt (line 5)) (25.12.19)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from tensorflow==2.18.0->-r ../requirements.txt (line 5)) (0.7.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from tensorflow==2.18.0->-r ../requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from tensorflow==2.18.0->-r ../requirements.txt (line 5)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from tensorflow==2.18.0->-r ../requirements.txt (line 5)) (3.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from tensorflow==2.18.0->-r ../requirements.txt (line 5)) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from tensorflow==2.18.0->-r ../requirements.txt (line 5)) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from tensorflow==2.18.0->-r ../requirements.txt (line 5)) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from tensorflow==2.18.0->-r ../requirements.txt (line 5)) (2.0.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from tensorflow==2.18.0->-r ../requirements.txt (line 5)) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from tensorflow==2.18.0->-r ../requirements.txt (line 5)) (3.13.1)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from tensorflow==2.18.0->-r ../requirements.txt (line 5)) (3.15.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from tensorflow==2.18.0->-r ../requirements.txt (line 5)) (0.4.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from torch==2.6.0->-r ../requirements.txt (line 7)) (3.20.3)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from torch==2.6.0->-r ../requirements.txt (line 7)) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from torch==2.6.0->-r ../requirements.txt (line 7)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from torch==2.6.0->-r ../requirements.txt (line 7)) (2026.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from torch==2.6.0->-r ../requirements.txt (line 7)) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from transformers==4.45.0->-r ../requirements.txt (line 9)) (0.36.0)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from transformers==4.45.0->-r ../requirements.txt (line 9)) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from transformers==4.45.0->-r ../requirements.txt (line 9)) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from transformers==4.45.0->-r ../requirements.txt (line 9)) (4.67.1)\n",
      "Requirement already satisfied: cachetools<6,>=3.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (5.5.2)\n",
      "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (0.5.31)\n",
      "Requirement already satisfied: google-auth-httplib2<0.3.0,>=0.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (0.2.1)\n",
      "Requirement already satisfied: google-cloud-datastore<3,>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2.23.0)\n",
      "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2.34.0)\n",
      "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (1.13.0)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.18.2 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2.19.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<4,>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (3.40.0)\n",
      "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2.36.0)\n",
      "Requirement already satisfied: google-cloud-bigtable<3,>=2.19.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2.35.0)\n",
      "Requirement already satisfied: google-cloud-spanner<4,>=3.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (3.62.0)\n",
      "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (3.34.0)\n",
      "Requirement already satisfied: google-cloud-language<3,>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2.19.0)\n",
      "Requirement already satisfied: google-cloud-videointelligence<3,>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2.18.0)\n",
      "Requirement already satisfied: google-cloud-vision<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (3.12.0)\n",
      "Requirement already satisfied: google-cloud-recommendations-ai<0.11.0,>=0.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (0.10.18)\n",
      "Requirement already satisfied: google-cloud-aiplatform<2.0,>=1.26.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (1.134.0)\n",
      "Requirement already satisfied: keyrings.google-artifactregistry-auth in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (1.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from sympy==1.13.1->torch==2.6.0->-r ../requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-cloud-firestore==2.23.0->-r ../requirements.txt (line 2)) (1.72.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-cloud-firestore==2.23.0->-r ../requirements.txt (line 2)) (1.65.5)\n",
      "Requirement already satisfied: oauth2client>=1.4.12 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (4.1.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-firestore==2.23.0->-r ../requirements.txt (line 2)) (0.4.2)\n",
      "Requirement already satisfied: cryptography>=38.0.3 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-firestore==2.23.0->-r ../requirements.txt (line 2)) (46.0.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-firestore==2.23.0->-r ../requirements.txt (line 2)) (4.9.1)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.59.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (1.59.0)\n",
      "Requirement already satisfied: pydantic<3 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2.12.5)\n",
      "Requirement already satisfied: docstring_parser<1 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (0.17.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2.8.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.12.4 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (0.14.3)\n",
      "Requirement already satisfied: google-crc32c<2.0.0dev,>=1.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.27.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.27.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (1.39.1)\n",
      "Requirement already satisfied: overrides<8.0.0,>=6.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (7.7.0)\n",
      "Requirement already satisfied: sqlparse>=0.4.4 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (0.5.5)\n",
      "Requirement already satisfied: grpc-interceptor>=0.15.4 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (0.15.4)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions>=0.43b0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (0.60b1)\n",
      "Requirement already satisfied: opentelemetry-resourcedetector-gcp>=1.8.0a0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (1.11.0a0)\n",
      "Requirement already satisfied: google-cloud-monitoring>=2.16.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2.29.0)\n",
      "Requirement already satisfied: mmh3>=4.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (5.2.0)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.59.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (4.12.1)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.59.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.59.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.59.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (15.0.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.59.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.59.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.59.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (3.11)\n",
      "Requirement already satisfied: docopt in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (0.6.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from httplib2<0.23.0,>=0.8->apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.59.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.59.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.59.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0->-r ../requirements.txt (line 9)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (0.30.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (0.4.2)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=2.6.1 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from pymongo<5.0.0,>=3.8.0->apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2.8.0)\n",
      "Requirement already satisfied: PyJWT>=2.9.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from redis<6,>=5.0.0->apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2.10.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from requests<3.0.0,>=2.24.0->apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from requests<3.0.0,>=2.24.0->apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2.6.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-firestore==2.23.0->-r ../requirements.txt (line 2)) (0.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0->-r ../requirements.txt (line 5)) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0->-r ../requirements.txt (line 5)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0->-r ../requirements.txt (line 5)) (3.1.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.18.0->-r ../requirements.txt (line 5)) (0.45.1)\n",
      "Requirement already satisfied: pycparser in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from cffi>=1.0->soundfile==0.13.1->-r ../requirements.txt (line 4)) (3.0)\n",
      "Requirement already satisfied: rich in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.18.0->-r ../requirements.txt (line 5)) (14.2.0)\n",
      "Requirement already satisfied: namex in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.18.0->-r ../requirements.txt (line 5)) (0.1.0)\n",
      "Requirement already satisfied: optree in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.18.0->-r ../requirements.txt (line 5)) (0.18.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (8.7.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (3.23.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.0->-r ../requirements.txt (line 5)) (3.0.3)\n",
      "Requirement already satisfied: keyring in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (25.7.0)\n",
      "Requirement already satisfied: pluggy in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (1.6.0)\n",
      "Requirement already satisfied: jaraco.classes in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: jaraco.functools in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: jaraco.context in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (6.1.0)\n",
      "Requirement already satisfied: more-itertools in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from jaraco.classes->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (10.8.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow==2.18.0->-r ../requirements.txt (line 5)) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow==2.18.0->-r ../requirements.txt (line 5)) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.18.0->-r ../requirements.txt (line 5)) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51cfc085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "import logging\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6b48dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logs\n",
    "logging.basicConfig(level=logging.INFO, format=\"edem_apache_beam-%(message)s\")\n",
    "\n",
    "# Set Apache Beam log level to WARNING\n",
    "logging.getLogger('apache_beam').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab083ece",
   "metadata": {},
   "source": [
    "## Apache Beam Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e0c12",
   "metadata": {},
   "source": [
    "Apache Beam is a unified programming model for parallel data processing that provides a rich set of transformations to efficiently manipulate and process both batch and streaming data across multiple execution environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b0f435",
   "metadata": {},
   "source": [
    "### **Apache Beam Core Concepts** \n",
    "\n",
    "##### **Pipeline**\n",
    "A **Pipeline** represents the entire workflow for processing data in Apache Beam. It defines the sequence of transformations applied to input data to produce outputs.\n",
    "\n",
    "##### **PCollection**\n",
    "A **PCollection** is an immutable, distributed dataset that serves as the input and output for transformations in a pipeline. It can handle bounded (batch) or unbounded (streaming) data.\n",
    "\n",
    "##### **Transformations**\n",
    "A **Transformation** is an operation applied to a PCollection to produce one or more output PCollections. Common transformations include Map, FlatMap, GroupByKey, and Combine.\n",
    "\n",
    "##### **PTransform**\n",
    "A **PTransform** is a reusable and composable abstraction that encapsulates one or more transformations. It simplifies pipelines by modularizing complex workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aac87ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edem_apache_beam-Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edem_apache_beam-Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edem_apache_beam-Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edem_apache_beam-1\n",
      "edem_apache_beam-2\n",
      "edem_apache_beam-3\n",
      "edem_apache_beam-4\n",
      "edem_apache_beam-5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exercise 01: What is a PCollection?\n",
    "\n",
    "PCollection: A distributed dataset in Apache Beam\n",
    "\"\"\"\n",
    "\n",
    "#  Create and explore a simple PCollection\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "    \n",
    "    # Create a PCollection\n",
    "    numbers = pipeline | \"Create Numbers\" >> beam.Create([1, 2, 3, 4, 5])\n",
    "\n",
    "    # Log output\n",
    "    numbers | \"Log Elements\" >> beam.Map(logging.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42738f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edem_apache_beam-1\n",
      "edem_apache_beam-4\n",
      "edem_apache_beam-9\n",
      "edem_apache_beam-16\n",
      "edem_apache_beam-25\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exercise 02: Declaring a Pipeline\n",
    "\n",
    "Pipeline: Defines the workflow of data transformations.\n",
    "\"\"\"\n",
    "\n",
    "# Interactive pipeline\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "    \n",
    "    # Create a PCollection and perform a transformation\n",
    "    squared_numbers = (\n",
    "        pipeline\n",
    "        | \"Create Numbers\" >> beam.Create([1, 2, 3, 4, 5])\n",
    "        | \"Square Numbers\" >> beam.Map(lambda x: x * x)\n",
    "    )\n",
    "\n",
    "    # Log output\n",
    "    squared_numbers | \"Log Squared Numbers\" >> beam.Map(logging.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21c88277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edem_apache_beam-('En', 1)\n",
      "edem_apache_beam-('un', 2)\n",
      "edem_apache_beam-('lugar', 1)\n",
      "edem_apache_beam-('de', 12)\n",
      "edem_apache_beam-('la', 1)\n",
      "edem_apache_beam-('Mancha,', 1)\n",
      "edem_apache_beam-('cuyo', 1)\n",
      "edem_apache_beam-('nombre', 1)\n",
      "edem_apache_beam-('no', 2)\n",
      "edem_apache_beam-('quiero', 1)\n",
      "edem_apache_beam-('acordarme,', 1)\n",
      "edem_apache_beam-('ha', 1)\n",
      "edem_apache_beam-('mucho', 1)\n",
      "edem_apache_beam-('tiempo', 1)\n",
      "edem_apache_beam-('que', 2)\n",
      "edem_apache_beam-('vivía', 1)\n",
      "edem_apache_beam-('hidalgo', 1)\n",
      "edem_apache_beam-('los', 5)\n",
      "edem_apache_beam-('lanza', 1)\n",
      "edem_apache_beam-('en', 1)\n",
      "edem_apache_beam-('astillero,', 1)\n",
      "edem_apache_beam-('adarga', 1)\n",
      "edem_apache_beam-('antigua,', 1)\n",
      "edem_apache_beam-('rocín', 1)\n",
      "edem_apache_beam-('flaco', 1)\n",
      "edem_apache_beam-('y', 2)\n",
      "edem_apache_beam-('galgo', 1)\n",
      "edem_apache_beam-('corredor.', 1)\n",
      "edem_apache_beam-('Una', 1)\n",
      "edem_apache_beam-('olla', 1)\n",
      "edem_apache_beam-('algo', 1)\n",
      "edem_apache_beam-('más', 3)\n",
      "edem_apache_beam-('vaca', 1)\n",
      "edem_apache_beam-('carnero,', 1)\n",
      "edem_apache_beam-('salpicón', 1)\n",
      "edem_apache_beam-('las', 3)\n",
      "edem_apache_beam-('noches,', 1)\n",
      "edem_apache_beam-('duelos', 1)\n",
      "edem_apache_beam-('quebrantos', 1)\n",
      "edem_apache_beam-('sábados,', 1)\n",
      "edem_apache_beam-('lantejas', 1)\n",
      "edem_apache_beam-('viernes,', 1)\n",
      "edem_apache_beam-('algún', 1)\n",
      "edem_apache_beam-('palomino', 1)\n",
      "edem_apache_beam-('añadidura', 1)\n",
      "edem_apache_beam-('domingos,', 1)\n",
      "edem_apache_beam-('consumían', 1)\n",
      "edem_apache_beam-('tres', 1)\n",
      "edem_apache_beam-('partes', 1)\n",
      "edem_apache_beam-('su', 2)\n",
      "edem_apache_beam-('hacienda.', 1)\n",
      "edem_apache_beam-('El', 1)\n",
      "edem_apache_beam-('resto', 1)\n",
      "edem_apache_beam-('della', 1)\n",
      "edem_apache_beam-('concluían', 1)\n",
      "edem_apache_beam-('sayo', 1)\n",
      "edem_apache_beam-('velarte,', 1)\n",
      "edem_apache_beam-('calzas', 1)\n",
      "edem_apache_beam-('velludo', 1)\n",
      "edem_apache_beam-('para', 1)\n",
      "edem_apache_beam-('fiestas', 1)\n",
      "edem_apache_beam-('con', 2)\n",
      "edem_apache_beam-('sus', 1)\n",
      "edem_apache_beam-('pantuflos', 1)\n",
      "edem_apache_beam-('lo', 2)\n",
      "edem_apache_beam-('mismo,', 1)\n",
      "edem_apache_beam-('días', 1)\n",
      "edem_apache_beam-('entre', 1)\n",
      "edem_apache_beam-('semana', 1)\n",
      "edem_apache_beam-('se', 1)\n",
      "edem_apache_beam-('honraba', 1)\n",
      "edem_apache_beam-('vellorí', 1)\n",
      "edem_apache_beam-('fino.', 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exercise 03: Transformations\n",
    "\n",
    "Transforms one or more input PCollections into one or more output PCollections.\n",
    "\"\"\"\n",
    "\n",
    "# Interactive pipeline\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            # Input\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText(\n",
    "                './00_DocAux/input_text.txt')\n",
    "            # Transformations\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"Map\" >> beam.Map(lambda x: (x,1))\n",
    "            | \"Combine\" >> beam.CombinePerKey(sum)\n",
    "            # Output\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13087301",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 04: Reuse Transformations (PTransforms)\n",
    "\n",
    "PTransform: A collection of transformations packaged as a reusable unit for complex workflows.\n",
    "\"\"\"\n",
    "\n",
    "class MyCustomTransform(beam.PTransform):\n",
    "    \n",
    "    def expand(self, pcoll):\n",
    "        return (\n",
    "            pcoll\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"Map\" >> beam.Map(lambda x: (x,1))\n",
    "            | \"Combine\" >> beam.CombinePerKey(sum)\n",
    "        )\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./00_DocAux/input_text.txt')\n",
    "            | \"Apply Custom Transform\" >> MyCustomTransform()\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32529521",
   "metadata": {},
   "source": [
    "#### **Apache Beam Transformations** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378df144",
   "metadata": {},
   "source": [
    "**A. GroupByKey**\n",
    "\n",
    "It groups the elements of a data bundle by a common key, producing a collection where each key is unique and associated with a list of values that share that key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9b9d9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edem_apache_beam-('Spain', ['Valencia', 'Barcelona'])\n",
      "edem_apache_beam-('France', ['Paris'])\n"
     ]
    }
   ],
   "source": [
    "\"\"\" GroupByKey \"\"\"\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    data = (p | \"PCollection\" >> beam.Create([('Spain', 'Valencia'), ('Spain','Barcelona'), ('France', 'Paris')]))\n",
    "\n",
    "    (data \n",
    "        | \"Combined\" >> beam.GroupByKey()\n",
    "        | \"Print\" >> beam.Map(logging.info))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4d4b26",
   "metadata": {},
   "source": [
    "**B. CoGroupByKey** \n",
    "\n",
    "It merges two PCollections by key, producing pairs where each key is associated with lists of elements from both input collections. This is useful for operations involving data from two different sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55604616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def fun(element):\n",
    "    country,city = element\n",
    "    if country == 'Spain' :\n",
    "        for city in country:\n",
    "            if city == 'Valencia':\n",
    "                return city\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    p1 = p | \"PCollection 01\" >> beam.Create([('Spain', 'Valencia'), ('Spain','Barcelona'), ('France', 'Paris')])\n",
    "    p2 = p | \"PCollection 02\" >> beam.Create([('Spain', 'Madrid'), ('Spain','Alicante'), ('France', 'Lyon')])\n",
    "\n",
    "    data = ((p1,p2) | beam.CoGroupByKey())\n",
    "    bestcity = data | beam.Map(fun)\n",
    "    bestcity | beam.Map(print)\n",
    "\n",
    "    # data | \"Print\" >> beam.Map(logging.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e798a5",
   "metadata": {},
   "source": [
    "**C. Flatten**  \n",
    "\n",
    "It merges multiple PCollections into a single PCollection, combining their elements into one flat structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd9f59e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madrid\n",
      "Barcelona\n",
      "Valencia\n",
      "Malaga\n",
      "New York\n",
      "Los Angeles\n",
      "Miami\n",
      "Chicago\n",
      "London\n",
      "Manchester\n",
      "Liverpool\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    p1 = p | \"PCollection 01\" >> beam.Create(['New York', 'Los Angeles', 'Miami', 'Chicago'])\n",
    "    p2 = p | \"Pcollection 02\" >> beam.Create(['Madrid', 'Barcelona', 'Valencia', 'Malaga'])\n",
    "    p3 = p | \"Pcollection 03\" >> beam.Create(['London','Manchester', 'Liverpool'])\n",
    "\n",
    "    merged = ((p1,p2,p3)| beam.Flatten())\n",
    "\n",
    "    merged | beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7986675c",
   "metadata": {},
   "source": [
    "**D. Partition**  \n",
    "\n",
    "It splits a PCollection into multiple partitions based on defined criteria, enabling parallel and distributed processing of subsets of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8448bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['Spain', 'USA', 'Switzerland']\n",
    "\n",
    "def choose_partition(city, num_partitions):\n",
    "    \n",
    "    usa = {'New York', 'Los Angeles', 'Miami', 'Chicago'}\n",
    "    eu  = {'Madrid', 'Barcelona', 'Valencia', 'Malaga'}\n",
    "    uk  = {'London', 'Manchester', 'Liverpool'}\n",
    "\n",
    "    if city in usa:\n",
    "        return 0  \n",
    "    if city in eu:\n",
    "        return 1  \n",
    "    if city in uk:\n",
    "        return 2 \n",
    "    return 3 \n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "        usa,eu,uk,others = (\n",
    "                p \n",
    "                | \"PCollection\" >> beam.Create([\n",
    "                        'New York', 'Los Angeles', 'Miami', 'Chicago',\n",
    "                        'Madrid', 'Barcelona', 'Valencia', 'Malaga',\n",
    "                        'London', 'Manchester', 'Liverpool','Tokyo' \n",
    "                ])\n",
    "                | \"partition\" >> beam.Partition(choose_partition, 4)\n",
    "        )\n",
    "\n",
    "        eu |  beam.Map(logging.info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34356117",
   "metadata": {},
   "source": [
    "## Introducing complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1fc54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 05: This exercise shows how to achieve the same transformation\n",
    "    using both Map and ParDo with DoFn.\n",
    "\n",
    "Use Map for simple transformations where the logic is inline and does not require lifecycle methods.\n",
    "Use ParDo with DoFn for more complex operations requiring setup, cleanup, or state management.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Map \"\"\"\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./00_DocAux/input_text.txt')\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"Map\" >> beam.Map(lambda x: x.upper())\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39217964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ParDo with DoFn\"\"\"\n",
    "\n",
    "class UpperCaseDoFn(beam.DoFn):\n",
    "\n",
    "    def setup(self):\n",
    "        logging.info(\"Loading model...\")\n",
    "        self.model = lambda x: x.upper()\n",
    "\n",
    "    def process(self, element):\n",
    "        yield self.model(element)\n",
    "\n",
    "    def teardown(self):\n",
    "        logging.info(\"Releasing model resources.\")\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./00_DocAux/input_text.txt')\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"DoFn\" >> beam.ParDo(UpperCaseDoFn())\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d7f06",
   "metadata": {},
   "source": [
    "### **Exploring the DoFn LifeCycle**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92b92b2",
   "metadata": {},
   "source": [
    "The Lifecycle of a **DoFn** in Apache Beam refers to the stages that an instance of the `DoFn` class goes through from initialization to completion within the context of a *ParDo* transformation. Below are the key phases of the DoFn life cycle:\n",
    "\n",
    "1. **Setup (Initialization):**  \n",
    "   - **Goal:** This phase occurs once for each instance of `DoFn` before processing begins.  \n",
    "   - **Process:** Initialization tasks, such as allocating resources, establishing connections, or loading models, are performed in the `setup(self)` method.\n",
    "\n",
    "2. **Processing Elements (Process):**  \n",
    "   - **Goal:** This phase executes the main logic for each input element in the data bundle.  \n",
    "   - **Process:** The core processing logic is implemented in the `process(self, element)` method. This method is called for each element in the input `PCollection` and defines how the data is transformed.\n",
    "\n",
    "3. **Start Bundle and Finish Bundle:**  \n",
    "   - **Goal:** These phases occur once before (`start_bundle(self)`) and after (`finish_bundle(self)`) processing all elements in a bundle. A bundle represents a chunk of data processed in parallel.  \n",
    "   - **Process:** Bundle-specific setup and cleanup tasks, such as initializing or finalizing temporary states or resources used within the bundle, are performed here.\n",
    "\n",
    "4. **Teardown (Finalization):**  \n",
    "   - **Goal:** This phase occurs once after all elements have been processed.  \n",
    "   - **Process:** The `teardown(self)` method handles final resource cleanup, such as closing database connections, releasing memory, or terminating external processes.\n",
    "\n",
    "---\n",
    "\n",
    "The lifecycle of a **DoFn** provides a structured framework for managing resources and processing logic in *ParDo* transformations. Each instance is created, initialized, processes elements, and performs cleanup in a controlled and efficient manner. This ensures proper resource management and facilitates robust, scalable data processing pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "528ec3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edem_apache_beam-Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edem_apache_beam-Setting up resources.\n",
      "edem_apache_beam-Starting a bundle.\n",
      "edem_apache_beam-Processing element: En\n",
      "edem_apache_beam-EN\n",
      "edem_apache_beam-Processing element: un\n",
      "edem_apache_beam-UN\n",
      "edem_apache_beam-Processing element: lugar\n",
      "edem_apache_beam-LUGAR\n",
      "edem_apache_beam-Processing element: de\n",
      "edem_apache_beam-DE\n",
      "edem_apache_beam-Processing element: la\n",
      "edem_apache_beam-LA\n",
      "edem_apache_beam-Processing element: Mancha,\n",
      "edem_apache_beam-MANCHA,\n",
      "edem_apache_beam-Processing element: de\n",
      "edem_apache_beam-DE\n",
      "edem_apache_beam-Processing element: cuyo\n",
      "edem_apache_beam-CUYO\n",
      "edem_apache_beam-Processing element: nombre\n",
      "edem_apache_beam-NOMBRE\n",
      "edem_apache_beam-Processing element: no\n",
      "edem_apache_beam-NO\n",
      "edem_apache_beam-Processing element: quiero\n",
      "edem_apache_beam-QUIERO\n",
      "edem_apache_beam-Processing element: acordarme,\n",
      "edem_apache_beam-ACORDARME,\n",
      "edem_apache_beam-Processing element: no\n",
      "edem_apache_beam-NO\n",
      "edem_apache_beam-Processing element: ha\n",
      "edem_apache_beam-HA\n",
      "edem_apache_beam-Processing element: mucho\n",
      "edem_apache_beam-MUCHO\n",
      "edem_apache_beam-Processing element: tiempo\n",
      "edem_apache_beam-TIEMPO\n",
      "edem_apache_beam-Processing element: que\n",
      "edem_apache_beam-QUE\n",
      "edem_apache_beam-Processing element: vivía\n",
      "edem_apache_beam-VIVÍA\n",
      "edem_apache_beam-Processing element: un\n",
      "edem_apache_beam-UN\n",
      "edem_apache_beam-Processing element: hidalgo\n",
      "edem_apache_beam-HIDALGO\n",
      "edem_apache_beam-Processing element: de\n",
      "edem_apache_beam-DE\n",
      "edem_apache_beam-Processing element: los\n",
      "edem_apache_beam-LOS\n",
      "edem_apache_beam-Processing element: de\n",
      "edem_apache_beam-DE\n",
      "edem_apache_beam-Processing element: lanza\n",
      "edem_apache_beam-LANZA\n",
      "edem_apache_beam-Processing element: en\n",
      "edem_apache_beam-EN\n",
      "edem_apache_beam-Processing element: astillero,\n",
      "edem_apache_beam-ASTILLERO,\n",
      "edem_apache_beam-Processing element: adarga\n",
      "edem_apache_beam-ADARGA\n",
      "edem_apache_beam-Processing element: antigua,\n",
      "edem_apache_beam-ANTIGUA,\n",
      "edem_apache_beam-Processing element: rocín\n",
      "edem_apache_beam-ROCÍN\n",
      "edem_apache_beam-Processing element: flaco\n",
      "edem_apache_beam-FLACO\n",
      "edem_apache_beam-Processing element: y\n",
      "edem_apache_beam-Y\n",
      "edem_apache_beam-Processing element: galgo\n",
      "edem_apache_beam-GALGO\n",
      "edem_apache_beam-Processing element: corredor.\n",
      "edem_apache_beam-CORREDOR.\n",
      "edem_apache_beam-Processing element: Una\n",
      "edem_apache_beam-UNA\n",
      "edem_apache_beam-Processing element: olla\n",
      "edem_apache_beam-OLLA\n",
      "edem_apache_beam-Processing element: de\n",
      "edem_apache_beam-DE\n",
      "edem_apache_beam-Processing element: algo\n",
      "edem_apache_beam-ALGO\n",
      "edem_apache_beam-Processing element: más\n",
      "edem_apache_beam-MÁS\n",
      "edem_apache_beam-Processing element: vaca\n",
      "edem_apache_beam-VACA\n",
      "edem_apache_beam-Processing element: que\n",
      "edem_apache_beam-QUE\n",
      "edem_apache_beam-Processing element: carnero,\n",
      "edem_apache_beam-CARNERO,\n",
      "edem_apache_beam-Processing element: salpicón\n",
      "edem_apache_beam-SALPICÓN\n",
      "edem_apache_beam-Processing element: las\n",
      "edem_apache_beam-LAS\n",
      "edem_apache_beam-Processing element: más\n",
      "edem_apache_beam-MÁS\n",
      "edem_apache_beam-Processing element: noches,\n",
      "edem_apache_beam-NOCHES,\n",
      "edem_apache_beam-Processing element: duelos\n",
      "edem_apache_beam-DUELOS\n",
      "edem_apache_beam-Processing element: y\n",
      "edem_apache_beam-Y\n",
      "edem_apache_beam-Processing element: quebrantos\n",
      "edem_apache_beam-QUEBRANTOS\n",
      "edem_apache_beam-Processing element: los\n",
      "edem_apache_beam-LOS\n",
      "edem_apache_beam-Processing element: sábados,\n",
      "edem_apache_beam-SÁBADOS,\n",
      "edem_apache_beam-Processing element: lantejas\n",
      "edem_apache_beam-LANTEJAS\n",
      "edem_apache_beam-Processing element: los\n",
      "edem_apache_beam-LOS\n",
      "edem_apache_beam-Processing element: viernes,\n",
      "edem_apache_beam-VIERNES,\n",
      "edem_apache_beam-Processing element: algún\n",
      "edem_apache_beam-ALGÚN\n",
      "edem_apache_beam-Processing element: palomino\n",
      "edem_apache_beam-PALOMINO\n",
      "edem_apache_beam-Processing element: de\n",
      "edem_apache_beam-DE\n",
      "edem_apache_beam-Processing element: añadidura\n",
      "edem_apache_beam-AÑADIDURA\n",
      "edem_apache_beam-Processing element: los\n",
      "edem_apache_beam-LOS\n",
      "edem_apache_beam-Processing element: domingos,\n",
      "edem_apache_beam-DOMINGOS,\n",
      "edem_apache_beam-Processing element: consumían\n",
      "edem_apache_beam-CONSUMÍAN\n",
      "edem_apache_beam-Processing element: las\n",
      "edem_apache_beam-LAS\n",
      "edem_apache_beam-Processing element: tres\n",
      "edem_apache_beam-TRES\n",
      "edem_apache_beam-Processing element: partes\n",
      "edem_apache_beam-PARTES\n",
      "edem_apache_beam-Processing element: de\n",
      "edem_apache_beam-DE\n",
      "edem_apache_beam-Processing element: su\n",
      "edem_apache_beam-SU\n",
      "edem_apache_beam-Processing element: hacienda.\n",
      "edem_apache_beam-HACIENDA.\n",
      "edem_apache_beam-Processing element: El\n",
      "edem_apache_beam-EL\n",
      "edem_apache_beam-Processing element: resto\n",
      "edem_apache_beam-RESTO\n",
      "edem_apache_beam-Processing element: della\n",
      "edem_apache_beam-DELLA\n",
      "edem_apache_beam-Processing element: concluían\n",
      "edem_apache_beam-CONCLUÍAN\n",
      "edem_apache_beam-Processing element: sayo\n",
      "edem_apache_beam-SAYO\n",
      "edem_apache_beam-Processing element: de\n",
      "edem_apache_beam-DE\n",
      "edem_apache_beam-Processing element: velarte,\n",
      "edem_apache_beam-VELARTE,\n",
      "edem_apache_beam-Processing element: calzas\n",
      "edem_apache_beam-CALZAS\n",
      "edem_apache_beam-Processing element: de\n",
      "edem_apache_beam-DE\n",
      "edem_apache_beam-Processing element: velludo\n",
      "edem_apache_beam-VELLUDO\n",
      "edem_apache_beam-Processing element: para\n",
      "edem_apache_beam-PARA\n",
      "edem_apache_beam-Processing element: las\n",
      "edem_apache_beam-LAS\n",
      "edem_apache_beam-Processing element: fiestas\n",
      "edem_apache_beam-FIESTAS\n",
      "edem_apache_beam-Processing element: con\n",
      "edem_apache_beam-CON\n",
      "edem_apache_beam-Processing element: sus\n",
      "edem_apache_beam-SUS\n",
      "edem_apache_beam-Processing element: pantuflos\n",
      "edem_apache_beam-PANTUFLOS\n",
      "edem_apache_beam-Processing element: de\n",
      "edem_apache_beam-DE\n",
      "edem_apache_beam-Processing element: lo\n",
      "edem_apache_beam-LO\n",
      "edem_apache_beam-Processing element: mismo,\n",
      "edem_apache_beam-MISMO,\n",
      "edem_apache_beam-Processing element: los\n",
      "edem_apache_beam-LOS\n",
      "edem_apache_beam-Processing element: días\n",
      "edem_apache_beam-DÍAS\n",
      "edem_apache_beam-Processing element: de\n",
      "edem_apache_beam-DE\n",
      "edem_apache_beam-Processing element: entre\n",
      "edem_apache_beam-ENTRE\n",
      "edem_apache_beam-Processing element: semana\n",
      "edem_apache_beam-SEMANA\n",
      "edem_apache_beam-Processing element: se\n",
      "edem_apache_beam-SE\n",
      "edem_apache_beam-Processing element: honraba\n",
      "edem_apache_beam-HONRABA\n",
      "edem_apache_beam-Processing element: con\n",
      "edem_apache_beam-CON\n",
      "edem_apache_beam-Processing element: su\n",
      "edem_apache_beam-SU\n",
      "edem_apache_beam-Processing element: vellorí\n",
      "edem_apache_beam-VELLORÍ\n",
      "edem_apache_beam-Processing element: de\n",
      "edem_apache_beam-DE\n",
      "edem_apache_beam-Processing element: lo\n",
      "edem_apache_beam-LO\n",
      "edem_apache_beam-Processing element: más\n",
      "edem_apache_beam-MÁS\n",
      "edem_apache_beam-Processing element: fino.\n",
      "edem_apache_beam-FINO.\n",
      "edem_apache_beam-Finishing a bundle.\n",
      "edem_apache_beam-Tearing down resources.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exercise: Understanding the lifecycle methods (setup, start_bundle, process, finish_bundle, and teardown)\n",
    " and how they are invoked during pipeline execution.\n",
    "\n",
    "Use ParDo with DoFn for tasks requiring state, lifecycle, or multiple outputs.\n",
    "\"\"\"\n",
    "\n",
    "class LifecycleDoFn(beam.DoFn):\n",
    "\n",
    "    def setup(self):\n",
    "        logging.info(\"Setting up resources.\")\n",
    "\n",
    "    def start_bundle(self):\n",
    "        logging.info(\"Starting a bundle.\")\n",
    "\n",
    "    def process(self, element):\n",
    "        logging.info(f\"Processing element: {element}\")\n",
    "        yield element.upper()\n",
    "\n",
    "    def finish_bundle(self):\n",
    "        logging.info(\"Finishing a bundle.\")\n",
    "\n",
    "    def teardown(self):\n",
    "        logging.info(\"Tearing down resources.\")\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./00_DocAux/input_text.txt')\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"Apply Lifecycle DoFn\" >> beam.ParDo(LifecycleDoFn())\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3b32a9",
   "metadata": {},
   "source": [
    "### **DoFn: Stateful Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda7b7e",
   "metadata": {},
   "source": [
    "- Allows operations that depend on previous or cumulative data.\n",
    "- Reduces the need to rely on external systems, such as databases or caching systems.\n",
    "- Efficient, as the state is stored locally for each key and window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32adfaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edem_apache_beam-Key coder FastPrimitivesCoder for transform <ParDo(PTransform) label=[[7]: Count Events Per Key]> with stateful DoFn may not be deterministic. This may cause incorrect behavior for complex key types. Consider adding an input type hint for this transform.\n",
      "edem_apache_beam-Key: en, Count: 1\n",
      "edem_apache_beam-Key: un, Count: 1\n",
      "edem_apache_beam-Key: lugar, Count: 1\n",
      "edem_apache_beam-Key: de, Count: 1\n",
      "edem_apache_beam-Key: la, Count: 1\n",
      "edem_apache_beam-Key: mancha,, Count: 1\n",
      "edem_apache_beam-Key: de, Count: 2\n",
      "edem_apache_beam-Key: cuyo, Count: 1\n",
      "edem_apache_beam-Key: nombre, Count: 1\n",
      "edem_apache_beam-Key: no, Count: 1\n",
      "edem_apache_beam-Key: quiero, Count: 1\n",
      "edem_apache_beam-Key: acordarme,, Count: 1\n",
      "edem_apache_beam-Key: no, Count: 2\n",
      "edem_apache_beam-Key: ha, Count: 1\n",
      "edem_apache_beam-Key: mucho, Count: 1\n",
      "edem_apache_beam-Key: tiempo, Count: 1\n",
      "edem_apache_beam-Key: que, Count: 1\n",
      "edem_apache_beam-Key: vivía, Count: 1\n",
      "edem_apache_beam-Key: un, Count: 2\n",
      "edem_apache_beam-Key: hidalgo, Count: 1\n",
      "edem_apache_beam-Key: de, Count: 3\n",
      "edem_apache_beam-Key: los, Count: 1\n",
      "edem_apache_beam-Key: de, Count: 4\n",
      "edem_apache_beam-Key: lanza, Count: 1\n",
      "edem_apache_beam-Key: en, Count: 2\n",
      "edem_apache_beam-Key: astillero,, Count: 1\n",
      "edem_apache_beam-Key: adarga, Count: 1\n",
      "edem_apache_beam-Key: antigua,, Count: 1\n",
      "edem_apache_beam-Key: rocín, Count: 1\n",
      "edem_apache_beam-Key: flaco, Count: 1\n",
      "edem_apache_beam-Key: y, Count: 1\n",
      "edem_apache_beam-Key: galgo, Count: 1\n",
      "edem_apache_beam-Key: corredor., Count: 1\n",
      "edem_apache_beam-Key: una, Count: 1\n",
      "edem_apache_beam-Key: olla, Count: 1\n",
      "edem_apache_beam-Key: de, Count: 5\n",
      "edem_apache_beam-Key: algo, Count: 1\n",
      "edem_apache_beam-Key: más, Count: 1\n",
      "edem_apache_beam-Key: vaca, Count: 1\n",
      "edem_apache_beam-Key: que, Count: 2\n",
      "edem_apache_beam-Key: carnero,, Count: 1\n",
      "edem_apache_beam-Key: salpicón, Count: 1\n",
      "edem_apache_beam-Key: las, Count: 1\n",
      "edem_apache_beam-Key: más, Count: 2\n",
      "edem_apache_beam-Key: noches,, Count: 1\n",
      "edem_apache_beam-Key: duelos, Count: 1\n",
      "edem_apache_beam-Key: y, Count: 2\n",
      "edem_apache_beam-Key: quebrantos, Count: 1\n",
      "edem_apache_beam-Key: los, Count: 2\n",
      "edem_apache_beam-Key: sábados,, Count: 1\n",
      "edem_apache_beam-Key: lantejas, Count: 1\n",
      "edem_apache_beam-Key: los, Count: 3\n",
      "edem_apache_beam-Key: viernes,, Count: 1\n",
      "edem_apache_beam-Key: algún, Count: 1\n",
      "edem_apache_beam-Key: palomino, Count: 1\n",
      "edem_apache_beam-Key: de, Count: 6\n",
      "edem_apache_beam-Key: añadidura, Count: 1\n",
      "edem_apache_beam-Key: los, Count: 4\n",
      "edem_apache_beam-Key: domingos,, Count: 1\n",
      "edem_apache_beam-Key: consumían, Count: 1\n",
      "edem_apache_beam-Key: las, Count: 2\n",
      "edem_apache_beam-Key: tres, Count: 1\n",
      "edem_apache_beam-Key: partes, Count: 1\n",
      "edem_apache_beam-Key: de, Count: 7\n",
      "edem_apache_beam-Key: su, Count: 1\n",
      "edem_apache_beam-Key: hacienda., Count: 1\n",
      "edem_apache_beam-Key: el, Count: 1\n",
      "edem_apache_beam-Key: resto, Count: 1\n",
      "edem_apache_beam-Key: della, Count: 1\n",
      "edem_apache_beam-Key: concluían, Count: 1\n",
      "edem_apache_beam-Key: sayo, Count: 1\n",
      "edem_apache_beam-Key: de, Count: 8\n",
      "edem_apache_beam-Key: velarte,, Count: 1\n",
      "edem_apache_beam-Key: calzas, Count: 1\n",
      "edem_apache_beam-Key: de, Count: 9\n",
      "edem_apache_beam-Key: velludo, Count: 1\n",
      "edem_apache_beam-Key: para, Count: 1\n",
      "edem_apache_beam-Key: las, Count: 3\n",
      "edem_apache_beam-Key: fiestas, Count: 1\n",
      "edem_apache_beam-Key: con, Count: 1\n",
      "edem_apache_beam-Key: sus, Count: 1\n",
      "edem_apache_beam-Key: pantuflos, Count: 1\n",
      "edem_apache_beam-Key: de, Count: 10\n",
      "edem_apache_beam-Key: lo, Count: 1\n",
      "edem_apache_beam-Key: mismo,, Count: 1\n",
      "edem_apache_beam-Key: los, Count: 5\n",
      "edem_apache_beam-Key: días, Count: 1\n",
      "edem_apache_beam-Key: de, Count: 11\n",
      "edem_apache_beam-Key: entre, Count: 1\n",
      "edem_apache_beam-Key: semana, Count: 1\n",
      "edem_apache_beam-Key: se, Count: 1\n",
      "edem_apache_beam-Key: honraba, Count: 1\n",
      "edem_apache_beam-Key: con, Count: 2\n",
      "edem_apache_beam-Key: su, Count: 2\n",
      "edem_apache_beam-Key: vellorí, Count: 1\n",
      "edem_apache_beam-Key: de, Count: 12\n",
      "edem_apache_beam-Key: lo, Count: 2\n",
      "edem_apache_beam-Key: más, Count: 3\n",
      "edem_apache_beam-Key: fino., Count: 1\n"
     ]
    }
   ],
   "source": [
    "from apache_beam.transforms.userstate import CombiningValueStateSpec\n",
    "\n",
    "\"\"\"\n",
    "Exercise: The state is persisted across multiple calls to process()\n",
    "\"\"\"\n",
    "\n",
    "class StatefulDoFn(beam.DoFn):\n",
    "    \n",
    "    # Define a state spec to maintain counts\n",
    "    state_spec = CombiningValueStateSpec(\"count\", sum)\n",
    "\n",
    "    def process(self, element, state=beam.DoFn.StateParam(state_spec)):\n",
    "        k,v = element\n",
    "        state.add(1)\n",
    "        yield f\"Key: {k}, Count: {state.read()}\"\n",
    "    \n",
    "# Interactive pipeline\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./00_DocAux/input_text.txt')\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"Map\" >> beam.Map(lambda x: (x.lower(),1))\n",
    "            | \"Count Events Per Key\" >> beam.ParDo(StatefulDoFn())\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df0cb7",
   "metadata": {},
   "source": [
    "### Beam ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b3c094",
   "metadata": {},
   "source": [
    "- The **RunInference** transform enables the execution of machine learning inference directly within an Apache Beam pipeline by applying a preconfigured model to incoming data elements\n",
    "\n",
    "- **MLTransform** is used within a pipeline to prepare data for machine learning model training. It encapsulates multiple data preprocessing operations into a single abstraction, enabling the application of diverse transformation workflows through a unified interfac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd8ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch --quiet\n",
    "!pip install tensorflow --quiet\n",
    "!pip install transformers==4.44.2 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f277aa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "from typing import Iterable\n",
    "from typing import Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForMaskedLM\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.ml.inference.base import KeyedModelHandler\n",
    "from apache_beam.ml.inference.base import PredictionResult\n",
    "from apache_beam.ml.inference.base import RunInference\n",
    "from apache_beam.ml.inference.huggingface_inference import HuggingFacePipelineModelHandler\n",
    "from apache_beam.ml.inference.huggingface_inference import HuggingFaceModelHandlerKeyedTensor\n",
    "from apache_beam.ml.inference.huggingface_inference import HuggingFaceModelHandlerTensor\n",
    "from apache_beam.ml.inference.huggingface_inference import PipelineTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "389cb5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edem_apache_beam-Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Your input_length: 245 is bigger than 0.9 * max_length: 20. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "edem_apache_beam-Translated Text: In a place of the Mancha, of which no one wants to agree, there is\n",
      "edem_apache_beam-BatchElements statistics: element_count=1 batch_count=1 next_batch_size=1 timings=[]\n"
     ]
    }
   ],
   "source": [
    "model_handler = HuggingFacePipelineModelHandler(\n",
    "    task=PipelineTask.Translation_XX_to_YY,\n",
    "    model = \"google/flan-t5-small\",\n",
    "    load_pipeline_args={'framework': 'pt', \"device\": -1},\n",
    "    inference_args={'max_new_tokens': 200}\n",
    ")\n",
    "\n",
    "class FormatOutput(beam.DoFn):\n",
    "    def process(self, element: PredictionResult[Dict]) -> Iterable[Tuple[str, str]]:\n",
    "        \n",
    "        translated_text = element.inference[0]['translation_text']\n",
    "\n",
    "        logging.info(f\"Translated Text: {translated_text}\")\n",
    "        \n",
    "        yield translated_text\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./00_DocAux/input_text.txt')\n",
    "            | \"RunInference\" >> RunInference(model_handler)\n",
    "            | \"Print\" >> beam.ParDo(FormatOutput())\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d6d503",
   "metadata": {},
   "source": [
    "### **Streaming** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2afee27",
   "metadata": {},
   "source": [
    "#### GCP Setup\n",
    "\n",
    "- PubSub Topics\n",
    "\n",
    "```\n",
    "gcloud pubsub topics create <TOPIC_NAME>\n",
    "```\n",
    "\n",
    "- PubSub Subscriptions\n",
    "\n",
    "```\n",
    "gcloud pubsub subscriptions create <SUBSCRIPTION_NAME> --topic=<TOPIC_NAME>\n",
    "```\n",
    "\n",
    "- Google Cloud Storage Bucket  \n",
    "\n",
    "```\n",
    "gcloud storage mb gs://<BUCKET_NAME> --location=<REGION_ID>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1da1a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "project_id = \"inspiring-bonus-481514-j4\"\n",
    "subscription_name = \"sub-pagaes\"\n",
    "topic_id = \"topicspotify\"\n",
    "bq_dataset = \"edem_data\"\n",
    "bq_table = \"streamingtb\"\n",
    "bucket_name = \"bucket-de-pau-para-beam\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbae5c76",
   "metadata": {},
   "source": [
    "**Window in Apache Beam**\n",
    "  \n",
    "A window in Apache Beam defines a time frame for **organizing and grouping data elements** during processing, enabling time-based operations.\n",
    "\n",
    "---\n",
    "\n",
    "**Types of Windows:**\n",
    "\n",
    "- **Fixed Window:** \n",
    "\n",
    "  Divides elements into fixed time intervals, segmenting the PCollection into evenly spaced, time-based windows.\n",
    "\n",
    "- **Sliding Window:**  \n",
    "\n",
    "  Creates overlapping windows with a specified size and stride, enabling continuous analysis of data over time.\n",
    "\n",
    "- **Session Window:**  \n",
    "\n",
    "  Groups elements based on **contiguous temporal activity**, where windows are dynamically defined by an **inactivity gap** between events, making it ideal for capturing logical sessions in streaming data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911fece6",
   "metadata": {},
   "source": [
    "**A. FixedWindows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3af993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[0.0, 300.0)', ['event1', 'event2'])\n",
      "('[300.0, 600.0)', ['event3'])\n"
     ]
    }
   ],
   "source": [
    "from apache_beam.transforms.window import FixedWindows\n",
    "\n",
    "class getWindowDoFn(beam.DoFn):\n",
    "    def process(self, element, window=beam.DoFn.WindowParam):\n",
    "        yield (str(window), element)\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "    events = [\n",
    "        beam.window.TimestampedValue(\"event1\", 60),   \n",
    "        beam.window.TimestampedValue(\"event2\", 180),  \n",
    "        beam.window.TimestampedValue(\"event3\", 360), \n",
    "    ]\n",
    "\n",
    "    (\n",
    "        p\n",
    "        | \"Create PCollection\" >> beam.Create(events)\n",
    "        | \"Sliding Window\" >> beam.WindowInto(FixedWindows(size=300))\n",
    "        | \"k,v\" >> beam.ParDo(getWindowDoFn())\n",
    "        | \"Combine\" >> beam.GroupByKey()\n",
    "        | \"Print\" >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2087a3ad",
   "metadata": {},
   "source": [
    "**B. SlidingWindows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41ed0785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[0.0, 10.0)', ['event1', 'event2', 'event3'])\n",
      "('[-2.0, 8.0)', ['event1', 'event2'])\n",
      "('[-4.0, 6.0)', ['event1'])\n",
      "('[-6.0, 4.0)', ['event1'])\n",
      "('[-8.0, 2.0)', ['event1'])\n",
      "('[6.0, 16.0)', ['event2', 'event3'])\n",
      "('[4.0, 14.0)', ['event2', 'event3'])\n",
      "('[2.0, 12.0)', ['event2', 'event3'])\n",
      "('[8.0, 18.0)', ['event3'])\n"
     ]
    }
   ],
   "source": [
    "from apache_beam.transforms.window import SlidingWindows\n",
    "\n",
    "class getWindowDoFn(beam.DoFn):\n",
    "    def process(self, element, window=beam.DoFn.WindowParam):\n",
    "            yield (str(window), element)\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "    events = [\n",
    "        beam.window.TimestampedValue(\"event1\", 1),   \n",
    "        beam.window.TimestampedValue(\"event2\", 7),  \n",
    "        beam.window.TimestampedValue(\"event3\", 9), \n",
    "    ]\n",
    "\n",
    "    (\n",
    "        p\n",
    "        | \"Create PCollection\" >> beam.Create(events)\n",
    "        | \"Sliding Window\" >> beam.WindowInto(SlidingWindows(size=10, period=2))\n",
    "        | \"k,v\" >> beam.ParDo(getWindowDoFn())\n",
    "        | \"Combine\" >> beam.GroupByKey()\n",
    "        | \"Print\" >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be21097f",
   "metadata": {},
   "source": [
    "**C. SessionWindows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddf567b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('key', ['event1', 'event2'])\n",
      "('key', ['event3', 'event4'])\n"
     ]
    }
   ],
   "source": [
    "from apache_beam.transforms.window import Sessions\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "    events = [\n",
    "        beam.window.TimestampedValue(\"event1\", 0),   \n",
    "        beam.window.TimestampedValue(\"event2\", 5),  \n",
    "        beam.window.TimestampedValue(\"event3\", 25),\n",
    "        beam.window.TimestampedValue(\"event4\", 27),\n",
    "    ]\n",
    "\n",
    "    (\n",
    "        p\n",
    "        | \"Create PCollection\" >> beam.Create(events)\n",
    "        | \"Sliding Window\" >> beam.WindowInto(Sessions(gap_size=7))\n",
    "        | \"k,v\" >> beam.Map(lambda x: (\"key\", x))\n",
    "        | \"Combine\" >> beam.GroupByKey()\n",
    "        | \"Print\" >> beam.Map(print)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874fa2e3",
   "metadata": {},
   "source": [
    "**D. Triggers, Watermark & Allowed Lateness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3736151e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.transforms.core:[19]: Group: Unsafe trigger `AfterWatermark(early=AfterProcessingTime(delay=3))` detected (reason: MAY_FINISH). This is being allowed because --allow_unsafe_triggers is set. This could lead to missing or incomplete groups.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('key', ['event1', 'event2'])\n",
      "('key', ['event1', 'event2', 'event3'])\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.testing.test_stream import TestStream\n",
    "from apache_beam.transforms.window import FixedWindows\n",
    "from apache_beam.transforms.trigger import AfterWatermark, AfterProcessingTime\n",
    "from apache_beam.transforms.trigger import AccumulationMode\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "opts = PipelineOptions(flags=[\"--allow_unsafe_triggers\"],streaming=True, runner=\"DirectRunner\")\n",
    "\n",
    "with beam.Pipeline(options=opts) as p:\n",
    "    events = (\n",
    "        TestStream()\n",
    "        .add_elements([\n",
    "            beam.window.TimestampedValue(\"event1\", 0),\n",
    "            beam.window.TimestampedValue(\"event2\", 2),\n",
    "        ])\n",
    "        .advance_processing_time(5)\n",
    "        .add_elements([\n",
    "            beam.window.TimestampedValue(\"event3\", 4),\n",
    "        ])\n",
    "        .advance_watermark_to(10)\n",
    "        .add_elements([\n",
    "            beam.window.TimestampedValue(\"late_event\", 1),\n",
    "        ])\n",
    "        .advance_watermark_to_infinity()\n",
    "    )\n",
    "\n",
    "    (\n",
    "        p\n",
    "        | \"Stream\" >> events\n",
    "        | \"Window\" >> beam.WindowInto(\n",
    "            FixedWindows(5),\n",
    "            trigger=AfterWatermark(early=AfterProcessingTime(3)),\n",
    "            allowed_lateness=10,\n",
    "            accumulation_mode=AccumulationMode.ACCUMULATING\n",
    "        )\n",
    "        | \"k,v\" >> beam.Map(lambda x: (\"key\", x))\n",
    "        | \"Group\" >> beam.GroupByKey()\n",
    "        | \"Print\" >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8161b20",
   "metadata": {},
   "source": [
    "### Apache Beam on Google Cloud: Dataflow Pipeline from Pub/Sub to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b04b5418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.runners.dataflow.dataflow_runner:2026-01-28T14:29:03.779Z: JOB_MESSAGE_WARNING: Autoscaling is enabled for Dataflow Streaming Engine. Workers will scale between 1 and 100 unless maxNumWorkers is specified.\n",
      "WARNING:apache_beam.runners.dataflow.dataflow_runner:2026-01-28T14:29:10.193Z: JOB_MESSAGE_WARNING: Streaming job has set up its own fixed sharding configuration. Liquid sharding will be disabled. Parallelism will be set to default\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Run Process\u001b[39;00m\n\u001b[32m     40\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[33mThe process started\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbeam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPipelineOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43medem-pubsub-to-bigquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDataflowRunner\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemp_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgs://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbucket_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/tmp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstaging_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgs://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbucket_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/staging\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meurope-southwest1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m            \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m            \u001b[49m\u001b[43m|\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mReadFromPubSub\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m>>\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mReadFromPubSub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubscription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprojects/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mproject_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/subscriptions/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msubscription_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages/apache_beam/pipeline.py:622\u001b[39m, in \u001b[36mPipeline.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28mself\u001b[39m.result = \u001b[38;5;28mself\u001b[39m.run()\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._options.view_as(StandardOptions).no_wait_until_finish:\n\u001b[32m--> \u001b[39m\u001b[32m622\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_until_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    624\u001b[39m   logging.info(\n\u001b[32m    625\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mJob execution continues without waiting for completion.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    626\u001b[39m       \u001b[33m'\u001b[39m\u001b[33m Use \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwait_until_finish\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m in PipelineResult to block\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    627\u001b[39m       \u001b[33m'\u001b[39m\u001b[33m until finished.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:794\u001b[39m, in \u001b[36mDataflowPipelineResult.wait_until_finish\u001b[39m\u001b[34m(self, duration)\u001b[39m\n\u001b[32m    792\u001b[39m thread.start()\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m thread.is_alive():\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m   \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[38;5;66;03m# TODO: Merge the termination code in poll_for_job_completion and\u001b[39;00m\n\u001b[32m    797\u001b[39m \u001b[38;5;66;03m# is_in_terminal_state.\u001b[39;00m\n\u001b[32m    798\u001b[39m terminated = \u001b[38;5;28mself\u001b[39m.is_in_terminal_state()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import apache_beam as beam\n",
    "import logging\n",
    "import json\n",
    "import sys\n",
    "\n",
    "sys.argv = ['']\n",
    "\n",
    "def decode_message(msg):\n",
    "\n",
    "    output = msg.decode('utf-8')\n",
    "    logging.info(\"New PubSub Message: %s\", output)\n",
    "\n",
    "    return json.loads(output)\n",
    "\n",
    "def run():\n",
    "    with beam.Pipeline(options=PipelineOptions(\n",
    "        streaming=True,\n",
    "        job_name = \"edem-pubsub-to-bigquery\",\n",
    "        project=project_id,\n",
    "        runner=\"DataflowRunner\",\n",
    "        temp_location=f\"gs://{bucket_name}/tmp\",\n",
    "        staging_location=f\"gs://{bucket_name}/staging\",\n",
    "        region=\"europe-southwest1\"\n",
    "    )) as p:\n",
    "        \n",
    "        (\n",
    "            p \n",
    "            | \"ReadFromPubSub\" >> beam.io.ReadFromPubSub(subscription=f'projects/{project_id}/subscriptions/{subscription_name}')\n",
    "            | \"decode msg\" >> beam.Map(decode_message)\n",
    "            | \"Write to BigQuery\" >> beam.io.WriteToBigQuery(\n",
    "                table = f\"{project_id}:{bq_dataset}.{bq_table}\", # Required Format: PROJECT_ID:DATASET.TABLE\n",
    "                schema='name:STRING', # Required Format: field_name:TYPE\n",
    "                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Run Process\n",
    "logging.info(\"The process started\")\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f692f19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1769613218.900398 2051312 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publicado: 17874164903631430\n",
      "Publicado: 17874257231562902\n",
      "Publicado: 17874920121673807\n",
      "Publicado: 17874237761770942\n",
      "Publicado: 17874127834416000\n",
      "Publicado: 17875014050519396\n",
      "Publicado: 17874206818986979\n",
      "Publicado: 17874162259915790\n",
      "Publicado: 17874929923965472\n",
      "Publicado: 17874327892255312\n",
      "Publicado: 17874224861875693\n",
      "Publicado: 17874420189515946\n",
      "Publicado: 17875017751523641\n",
      "Publicado: 17874272817813310\n",
      "Publicado: 17875275287877847\n",
      "Publicado: 18014969195546491\n",
      "Publicado: 17874405418665553\n",
      "Publicado: 17874197552492475\n",
      "Publicado: 18014943135599445\n",
      "Publicado: 17874062254937543\n",
      "Publicado: 17875177285903068\n",
      "Publicado: 17874387240388705\n",
      "Publicado: 17874328380674853\n",
      "Publicado: 17874103172151759\n",
      "Publicado: 17873989934933671\n",
      "Publicado: 17874139169762070\n",
      "Publicado: 17874277159363101\n",
      "Publicado: 18014919198623021\n",
      "Publicado: 17874377237724721\n",
      "Publicado: 17874353489657331\n",
      "Publicado: 17874912590828660\n",
      "Publicado: 17874256134711984\n",
      "Publicado: 17874414445879050\n",
      "Publicado: 17874257231566373\n",
      "Publicado: 17874164903635835\n",
      "Publicado: 17874920121679062\n",
      "Publicado: 17874237761773928\n",
      "Publicado: 17874127834419544\n",
      "Publicado: 17875014050523673\n",
      "Publicado: 17874206818990001\n",
      "Publicado: 17874162259918941\n",
      "Publicado: 17874929923968614\n",
      "Publicado: 17874327892258687\n",
      "Publicado: 17874224861878922\n",
      "Publicado: 17874420189519783\n",
      "Publicado: 17875017751526650\n",
      "Publicado: 17874272817816149\n",
      "Publicado: 17875275287880775\n",
      "Publicado: 18014969195549686\n",
      "Publicado: 17874405418669191\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import pubsub_v1\n",
    "import json\n",
    "\n",
    "def publicarmensaje(nummensajes):\n",
    "    publisher = pubsub_v1.PublisherClient()\n",
    "    topic_path = publisher.topic_path(project_id, topic_id)\n",
    "    for i in range(nummensajes):\n",
    "        # Mensaje serializado a JSON y codificado a bytes\n",
    "        data = json.dumps({\"name\": f\"Pau{i+1}\"}).encode(\"utf-8\")\n",
    "        future = publisher.publish(topic_path, data)\n",
    "        print(f\"Publicado: {future.result()}\")\n",
    "\n",
    "publicarmensaje(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_def",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
