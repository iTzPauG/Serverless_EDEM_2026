{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c397ee4",
   "metadata": {},
   "source": [
    "## **Apache Beam Basics**\n",
    "\n",
    "Description: This notebook will teach you the basics of the Apache Beam Programming Model:\n",
    "\n",
    "- Apache Beam Basics: Pipeline, PCollection, Transforms & PTransforms.\n",
    "   - GroupByKey\n",
    "   - CoGroupByKey\n",
    "   - Flatten\n",
    "   - Partition.\n",
    "- Introducing complexity: DoFn.\n",
    "   - DoFn vs Map\n",
    "   - DoFn LifeCycle\n",
    "   - DoFn Stateful Processing\n",
    "- Advanced: Streaming.\n",
    "   - Fixed Windows\n",
    "   - Sliding Windows\n",
    "   - PubSub to Bigquery\n",
    "\n",
    "\n",
    "EDEM. Master Big Data & Cloud 2025/2026<br>\n",
    "Professor: Javi Briones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d25925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"apache-beam[gcp]\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a49fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apache-beam==2.62.0 (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached apache_beam-2.62.0-cp312-cp312-macosx_11_0_arm64.whl\n",
      "Collecting google-cloud-firestore==2.23.0 (from -r ../requirements.txt (line 2))\n",
      "  Using cached google_cloud_firestore-2.23.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas==2.3.3 (from -r ../requirements.txt (line 3))\n",
      "  Using cached pandas-2.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting soundfile==0.13.1 (from -r ../requirements.txt (line 4))\n",
      "  Using cached soundfile-0.13.1-py2.py3-none-macosx_11_0_arm64.whl.metadata (16 kB)\n",
      "Collecting tensorflow==2.18.0 (from -r ../requirements.txt (line 5))\n",
      "  Using cached tensorflow-2.18.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting crcmod==1.7 (from -r ../requirements.txt (line 6))\n",
      "  Using cached crcmod-1.7-cp312-cp312-macosx_11_0_arm64.whl\n",
      "Collecting torch==2.6.0 (from -r ../requirements.txt (line 7))\n",
      "  Using cached torch-2.6.0-cp312-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting torchaudio==2.6.0 (from -r ../requirements.txt (line 8))\n",
      "  Using cached torchaudio-2.6.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting transformers==4.45.0 (from -r ../requirements.txt (line 9))\n",
      "  Using cached transformers-4.45.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting orjson<4,>=3.9.7 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached orjson-3.11.5-cp312-cp312-macosx_15_0_arm64.whl.metadata (41 kB)\n",
      "Collecting dill<0.3.2,>=0.3.1.1 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached dill-0.3.1.1-py3-none-any.whl\n",
      "Collecting cloudpickle~=2.2.1 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting fastavro<2,>=0.23.6 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached fastavro-1.12.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (5.8 kB)\n",
      "Collecting fasteners<1.0,>=0.3 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached fasteners-0.20-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting grpcio!=1.48.0,!=1.59.*,!=1.60.*,!=1.61.*,!=1.62.0,!=1.62.1,<1.66.0,<2,>=1.33.1 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached grpcio-1.65.5-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.3 kB)\n",
      "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached hdfs-2.7.3-py3-none-any.whl\n",
      "Collecting httplib2<0.23.0,>=0.8 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jsonschema<5.0.0,>=4.0.0 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached jsonschema-4.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting jsonpickle<4.0.0,>=3.0.0 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached jsonpickle-3.4.2-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting numpy<2.3.0,>=1.14.3 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached numpy-2.2.6-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting objsize<0.8.0,>=0.6.1 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached objsize-0.7.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging>=22.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (25.0)\n",
      "Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached pymongo-4.16.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (10.0 kB)\n",
      "Collecting proto-plus<2,>=1.7.1 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached proto_plus-1.27.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<6.0.0.dev0,>=3.20.3 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting pydot<2,>=1.2.0 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached pydot-1.4.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (2.9.0.post0)\n",
      "Collecting pytz>=2018.3 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting redis<6,>=5.0.0 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached redis-5.3.1-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting regex>=2020.6.8 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached regex-2026.1.15-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting requests<3.0.0,>=2.24.0 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting sortedcontainers>=2.4.0 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1)) (4.15.0)\n",
      "Collecting zstandard<1,>=0.18.0 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached zstandard-0.25.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting pyyaml<7.0.0,>=3.12 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached pyyaml-6.0.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting pyarrow<17.0.0,>=3.0.0 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached pyarrow-16.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix<1 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-cloud-firestore==2.23.0->-r ../requirements.txt (line 2))\n",
      "  Using cached google_api_core-2.29.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 (from google-cloud-firestore==2.23.0->-r ../requirements.txt (line 2))\n",
      "  Using cached google_auth-2.47.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting google-cloud-core<3.0.0,>=1.4.1 (from google-cloud-firestore==2.23.0->-r ../requirements.txt (line 2))\n",
      "  Using cached google_cloud_core-2.5.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas==2.3.3->-r ../requirements.txt (line 3))\n",
      "  Using cached tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting cffi>=1.0 (from soundfile==0.13.1->-r ../requirements.txt (line 4))\n",
      "  Using cached cffi-2.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.6 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow==2.18.0->-r ../requirements.txt (line 5))\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow==2.18.0->-r ../requirements.txt (line 5))\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow==2.18.0->-r ../requirements.txt (line 5))\n",
      "  Using cached flatbuffers-25.12.19-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow==2.18.0->-r ../requirements.txt (line 5))\n",
      "  Using cached gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow==2.18.0->-r ../requirements.txt (line 5))\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow==2.18.0->-r ../requirements.txt (line 5))\n",
      "  Using cached libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow==2.18.0->-r ../requirements.txt (line 5))\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from tensorflow==2.18.0->-r ../requirements.txt (line 5)) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from tensorflow==2.18.0->-r ../requirements.txt (line 5)) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==2.18.0->-r ../requirements.txt (line 5))\n",
      "  Using cached termcolor-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow==2.18.0->-r ../requirements.txt (line 5))\n",
      "  Using cached wrapt-2.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow==2.18.0->-r ../requirements.txt (line 5))\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow==2.18.0->-r ../requirements.txt (line 5))\n",
      "  Using cached keras-3.13.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting numpy<2.3.0,>=1.14.3 (from apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached numpy-2.0.2-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow==2.18.0->-r ../requirements.txt (line 5))\n",
      "  Using cached h5py-3.15.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow==2.18.0->-r ../requirements.txt (line 5))\n",
      "  Using cached ml_dtypes-0.4.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting filelock (from torch==2.6.0->-r ../requirements.txt (line 7))\n",
      "  Using cached filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting networkx (from torch==2.6.0->-r ../requirements.txt (line 7))\n",
      "  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jinja2 (from torch==2.6.0->-r ../requirements.txt (line 7))\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch==2.6.0->-r ../requirements.txt (line 7))\n",
      "  Using cached fsspec-2026.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sympy==1.13.1 (from torch==2.6.0->-r ../requirements.txt (line 7))\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.45.0->-r ../requirements.txt (line 9))\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.0->-r ../requirements.txt (line 9))\n",
      "  Using cached tokenizers-0.20.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.45.0->-r ../requirements.txt (line 9))\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==4.45.0->-r ../requirements.txt (line 9))\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting cachetools<6,>=3.1.0 (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting google-apitools<0.5.32,>=0.5.31 (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_apitools-0.5.31-py3-none-any.whl\n",
      "Collecting google-auth-httplib2<0.3.0,>=0.1.0 (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_auth_httplib2-0.2.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-cloud-datastore<3,>=2.0.0 (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_cloud_datastore-2.23.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting google-cloud-pubsub<3,>=2.1.0 (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_cloud_pubsub-2.34.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting google-cloud-pubsublite<2,>=1.2.0 (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_cloud_pubsublite-1.13.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting google-cloud-storage<3,>=2.18.2 (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_cloud_storage-2.19.0-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting google-cloud-bigquery<4,>=2.0.0 (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_cloud_bigquery-3.40.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting google-cloud-bigquery-storage<3,>=2.6.3 (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_cloud_bigquery_storage-2.36.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting google-cloud-bigtable<3,>=2.19.0 (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_cloud_bigtable-2.35.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting google-cloud-spanner<4,>=3.0.0 (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_cloud_spanner-3.62.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting google-cloud-dlp<4,>=3.0.0 (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_cloud_dlp-3.34.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting google-cloud-language<3,>=2.0 (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_cloud_language-2.19.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting google-cloud-videointelligence<3,>=2.0 (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_cloud_videointelligence-2.18.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting google-cloud-vision<4,>=2 (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_cloud_vision-3.12.0-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting google-cloud-recommendations-ai<0.11.0,>=0.1.0 (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_cloud_recommendations_ai-0.10.18-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting google-cloud-aiplatform<2.0,>=1.26.0 (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_cloud_aiplatform-1.134.0-py2.py3-none-any.whl.metadata (46 kB)\n",
      "Collecting keyrings.google-artifactregistry-auth (from apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached keyrings.google_artifactregistry_auth-1.1.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch==2.6.0->-r ../requirements.txt (line 7))\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-cloud-firestore==2.23.0->-r ../requirements.txt (line 2))\n",
      "  Using cached googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-cloud-firestore==2.23.0->-r ../requirements.txt (line 2))\n",
      "  Using cached grpcio_status-1.76.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting oauth2client>=1.4.12 (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-firestore==2.23.0->-r ../requirements.txt (line 2))\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-firestore==2.23.0->-r ../requirements.txt (line 2))\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0,>=1.3.3 (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_cloud_resource_manager-1.16.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting google-genai<2.0.0,>=1.59.0 (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_genai-1.59.0-py3-none-any.whl.metadata (53 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting docstring_parser<1 (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting google-resumable-media<3.0.0,>=2.0.0 (from google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_resumable_media-2.8.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting grpc-google-iam-v1<1.0.0,>=0.12.4 (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached grpc_google_iam_v1-0.14.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting google-crc32c<2.0.0dev,>=1.5.0 (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_crc32c-1.8.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (1.7 kB)\n",
      "Collecting opentelemetry-api>=1.27.0 (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-sdk>=1.27.0 (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting overrides<8.0.0,>=6.0.1 (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting sqlparse>=0.4.4 (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached sqlparse-0.5.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting grpc-interceptor>=0.15.4 (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached grpc_interceptor-0.15.4-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting opentelemetry-semantic-conventions>=0.43b0 (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-resourcedetector-gcp>=1.8.0a0 (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached opentelemetry_resourcedetector_gcp-1.11.0a0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting google-cloud-monitoring>=2.16.0 (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_cloud_monitoring-2.29.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mmh3>=4.1.0 (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached mmh3-5.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (14 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 (from google-cloud-firestore==2.23.0->-r ../requirements.txt (line 2))\n",
      "  Using cached google_auth-2.48.0rc0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting anyio<5.0.0,>=4.8.0 (from google-genai<2.0.0,>=1.59.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting httpx<1.0.0,>=0.28.1 (from google-genai<2.0.0,>=1.59.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting tenacity<9.2.0,>=8.2.3 (from google-genai<2.0.0,>=1.59.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting websockets<15.1.0,>=13.0.0 (from google-genai<2.0.0,>=1.59.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached websockets-15.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting distro<2,>=1.7.0 (from google-genai<2.0.0,>=1.59.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting sniffio (from google-genai<2.0.0,>=1.59.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting idna>=2.8 (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.59.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting cryptography>=38.0.3 (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.59.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached cryptography-46.0.3-cp311-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-cloud-firestore==2.23.0->-r ../requirements.txt (line 2))\n",
      "  Using cached grpcio_status-1.75.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.75.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.70.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.69.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<0.23.0,>=0.8->apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached pyparsing-3.3.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting certifi (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.59.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.59.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.59.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0->-r ../requirements.txt (line 9))\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema<5.0.0,>=4.0.0->apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0.0,>=4.0.0->apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema<5.0.0,>=4.0.0->apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.25.0 (from jsonschema<5.0.0,>=4.0.0->apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached rpds_py-0.30.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached pydantic_core-2.41.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting dnspython<3.0.0,>=2.6.1 (from pymongo<5.0.0,>=3.8.0->apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting PyJWT>=2.9.0 (from redis<6,>=5.0.0->apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3.0.0,>=2.24.0->apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached charset_normalizer-3.4.4-cp312-cp312-macosx_10_13_universal2.whl.metadata (37 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.24.0->apache-beam==2.62.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-firestore==2.23.0->-r ../requirements.txt (line 2))\n",
      "  Using cached pyasn1-0.6.2-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow==2.18.0->-r ../requirements.txt (line 5))\n",
      "  Using cached markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow==2.18.0->-r ../requirements.txt (line 5))\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow==2.18.0->-r ../requirements.txt (line 5))\n",
      "  Using cached werkzeug-3.1.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.18.0->-r ../requirements.txt (line 5)) (0.45.1)\n",
      "Collecting pycparser (from cffi>=1.0->soundfile==0.13.1->-r ../requirements.txt (line 4))\n",
      "  Using cached pycparser-3.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow==2.18.0->-r ../requirements.txt (line 5))\n",
      "  Using cached rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow==2.18.0->-r ../requirements.txt (line 5))\n",
      "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow==2.18.0->-r ../requirements.txt (line 5))\n",
      "  Using cached optree-0.18.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Collecting importlib-metadata<8.8.0,>=6.0 (from opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached importlib_metadata-8.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting markupsafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.0->-r ../requirements.txt (line 5))\n",
      "  Using cached markupsafe-3.0.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Collecting keyring (from keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached keyring-25.7.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pluggy (from keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting jaraco.classes (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached jaraco.classes-3.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jaraco.functools (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached jaraco_functools-4.4.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting jaraco.context (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached jaraco_context-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting more-itertools (from jaraco.classes->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]==2.62.0->-r ../requirements.txt (line 1))\n",
      "  Using cached more_itertools-10.8.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow==2.18.0->-r ../requirements.txt (line 5))\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/Caskroom/miniconda/base/envs/env_def/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow==2.18.0->-r ../requirements.txt (line 5)) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.18.0->-r ../requirements.txt (line 5))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached google_cloud_firestore-2.23.0-py3-none-any.whl (411 kB)\n",
      "Using cached pandas-2.3.3-cp312-cp312-macosx_11_0_arm64.whl (10.7 MB)\n",
      "Using cached soundfile-0.13.1-py2.py3-none-macosx_11_0_arm64.whl (1.1 MB)\n",
      "Using cached tensorflow-2.18.0-cp312-cp312-macosx_12_0_arm64.whl (239.6 MB)\n",
      "Using cached torch-2.6.0-cp312-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "Using cached torchaudio-2.6.0-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached transformers-4.45.0-py3-none-any.whl (9.9 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Using cached fastavro-1.12.1-cp312-cp312-macosx_10_13_universal2.whl (1.0 MB)\n",
      "Using cached fasteners-0.20-py3-none-any.whl (18 kB)\n",
      "Using cached google_api_core-2.29.0-py3-none-any.whl (173 kB)\n",
      "Using cached google_auth_httplib2-0.2.1-py3-none-any.whl (9.5 kB)\n",
      "Using cached google_cloud_aiplatform-1.134.0-py2.py3-none-any.whl (8.2 MB)\n",
      "Using cached docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Using cached google_cloud_bigquery-3.40.0-py3-none-any.whl (261 kB)\n",
      "Using cached google_cloud_bigquery_storage-2.36.0-py3-none-any.whl (303 kB)\n",
      "Using cached google_cloud_bigtable-2.35.0-py3-none-any.whl (540 kB)\n",
      "Using cached google_cloud_core-2.5.0-py3-none-any.whl (29 kB)\n",
      "Using cached google_cloud_datastore-2.23.0-py3-none-any.whl (206 kB)\n",
      "Using cached google_cloud_dlp-3.34.0-py3-none-any.whl (220 kB)\n",
      "Using cached google_cloud_language-2.19.0-py3-none-any.whl (173 kB)\n",
      "Using cached google_cloud_pubsub-2.34.0-py3-none-any.whl (320 kB)\n",
      "Using cached google_cloud_pubsublite-1.13.0-py3-none-any.whl (324 kB)\n",
      "Using cached google_cloud_recommendations_ai-0.10.18-py3-none-any.whl (212 kB)\n",
      "Using cached google_cloud_resource_manager-1.16.0-py3-none-any.whl (400 kB)\n",
      "Using cached google_cloud_spanner-3.62.0-py3-none-any.whl (516 kB)\n",
      "Using cached google_cloud_storage-2.19.0-py2.py3-none-any.whl (131 kB)\n",
      "Using cached google_cloud_videointelligence-2.18.0-py3-none-any.whl (285 kB)\n",
      "Using cached google_cloud_vision-3.12.0-py3-none-any.whl (538 kB)\n",
      "Using cached google_crc32c-1.8.0-cp312-cp312-macosx_12_0_arm64.whl (31 kB)\n",
      "Using cached google_genai-1.59.0-py3-none-any.whl (719 kB)\n",
      "Using cached anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached google_auth-2.48.0rc0-py3-none-any.whl (236 kB)\n",
      "Using cached google_resumable_media-2.8.0-py3-none-any.whl (81 kB)\n",
      "Using cached googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "Using cached grpc_google_iam_v1-0.14.3-py3-none-any.whl (32 kB)\n",
      "Using cached grpcio-1.65.5-cp312-cp312-macosx_10_9_universal2.whl (10.4 MB)\n",
      "Using cached grpcio_status-1.65.5-py3-none-any.whl (14 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached jsonpickle-3.4.2-py3-none-any.whl (46 kB)\n",
      "Using cached jsonschema-4.26.0-py3-none-any.whl (90 kB)\n",
      "Using cached ml_dtypes-0.4.1-cp312-cp312-macosx_10_9_universal2.whl (405 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached numpy-2.0.2-cp312-cp312-macosx_14_0_arm64.whl (5.0 MB)\n",
      "Using cached objsize-0.7.1-py3-none-any.whl (11 kB)\n",
      "Using cached orjson-3.11.5-cp312-cp312-macosx_15_0_arm64.whl (129 kB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Using cached proto_plus-1.27.0-py3-none-any.whl (50 kB)\n",
      "Using cached protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl (418 kB)\n",
      "Using cached pyarrow-16.1.0-cp312-cp312-macosx_11_0_arm64.whl (26.0 MB)\n",
      "Using cached pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
      "Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Using cached pydantic_core-2.41.5-cp312-cp312-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Using cached pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Using cached pymongo-4.16.0-cp312-cp312-macosx_11_0_arm64.whl (917 kB)\n",
      "Using cached dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
      "Using cached pyparsing-3.3.2-py3-none-any.whl (122 kB)\n",
      "Using cached pyyaml-6.0.3-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached redis-5.3.1-py3-none-any.whl (272 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp312-cp312-macosx_10_13_universal2.whl (208 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached tokenizers-0.20.3-cp312-cp312-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Using cached urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
      "Using cached websockets-15.0.1-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached zstandard-0.25.0-cp312-cp312-macosx_11_0_arm64.whl (640 kB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "Using cached cffi-2.0.0-cp312-cp312-macosx_11_0_arm64.whl (181 kB)\n",
      "Using cached cryptography-46.0.3-cp311-abi3-macosx_10_9_universal2.whl (7.2 MB)\n",
      "Using cached flatbuffers-25.12.19-py2.py3-none-any.whl (26 kB)\n",
      "Using cached fsspec-2026.1.0-py3-none-any.whl (201 kB)\n",
      "Using cached gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Using cached google_cloud_monitoring-2.29.0-py3-none-any.whl (387 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpc_interceptor-0.15.4-py3-none-any.whl (20 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached h5py-3.15.1-cp312-cp312-macosx_11_0_arm64.whl (2.8 MB)\n",
      "Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Using cached keras-3.13.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "Using cached markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Using cached mmh3-5.2.0-cp312-cp312-macosx_11_0_arm64.whl (40 kB)\n",
      "Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Using cached opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n",
      "Using cached importlib_metadata-8.7.1-py3-none-any.whl (27 kB)\n",
      "Using cached opentelemetry_resourcedetector_gcp-1.11.0a0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached pyasn1-0.6.2-py3-none-any.whl (83 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached referencing-0.37.0-py3-none-any.whl (26 kB)\n",
      "Using cached regex-2026.1.15-cp312-cp312-macosx_11_0_arm64.whl (289 kB)\n",
      "Using cached rpds_py-0.30.0-cp312-cp312-macosx_11_0_arm64.whl (359 kB)\n",
      "Using cached safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl (447 kB)\n",
      "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Using cached sqlparse-0.5.5-py3-none-any.whl (46 kB)\n",
      "Using cached termcolor-3.3.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Using cached werkzeug-3.1.5-py3-none-any.whl (225 kB)\n",
      "Using cached markupsafe-3.0.3-cp312-cp312-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached wrapt-2.0.1-cp312-cp312-macosx_11_0_arm64.whl (61 kB)\n",
      "Using cached zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Using cached filelock-3.20.3-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached keyrings.google_artifactregistry_auth-1.1.2-py3-none-any.whl (10 kB)\n",
      "Using cached keyring-25.7.0-py3-none-any.whl (39 kB)\n",
      "Using cached jaraco.classes-3.4.0-py3-none-any.whl (6.8 kB)\n",
      "Using cached jaraco_context-6.1.0-py3-none-any.whl (7.1 kB)\n",
      "Using cached jaraco_functools-4.4.0-py3-none-any.whl (10 kB)\n",
      "Using cached more_itertools-10.8.0-py3-none-any.whl (69 kB)\n",
      "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "Using cached optree-0.18.0-cp312-cp312-macosx_11_0_arm64.whl (342 kB)\n",
      "Using cached pluggy-1.6.0-py3-none-any.whl (20 kB)\n",
      "Using cached pycparser-3.0-py3-none-any.whl (48 kB)\n",
      "Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: sortedcontainers, pytz, namex, mpmath, libclang, flatbuffers, docopt, crcmod, zstandard, zipp, wrapt, websockets, urllib3, tzdata, typing-inspection, tqdm, termcolor, tensorboard-data-server, tenacity, sympy, sqlparse, sniffio, safetensors, rpds-py, regex, pyyaml, pyparsing, PyJWT, pydantic-core, pycparser, pyasn1, pyarrow-hotfix, protobuf, pluggy, overrides, orjson, optree, opt-einsum, objsize, numpy, networkx, more-itertools, mmh3, mdurl, markupsafe, markdown, jsonpickle, jaraco.context, idna, hf-xet, h11, grpcio, google-pasta, google-crc32c, gast, fsspec, filelock, fasteners, fastavro, docstring_parser, dnspython, distro, dill, cloudpickle, charset_normalizer, certifi, cachetools, attrs, astunparse, annotated-types, absl-py, werkzeug, rsa, requests, referencing, redis, pymongo, pydot, pydantic, pyasn1-modules, pyarrow, proto-plus, pandas, ml-dtypes, markdown-it-py, jinja2, jaraco.functools, jaraco.classes, importlib-metadata, httplib2, httpcore, h5py, grpc-interceptor, googleapis-common-protos, google-resumable-media, cffi, anyio, torch, tensorboard, soundfile, rich, opentelemetry-api, oauth2client, keyring, jsonschema-specifications, huggingface-hub, httpx, hdfs, grpcio-status, cryptography, torchaudio, tokenizers, opentelemetry-semantic-conventions, keras, jsonschema, grpc-google-iam-v1, google-auth, google-apitools, transformers, tensorflow, opentelemetry-sdk, keyrings.google-artifactregistry-auth, google-auth-httplib2, google-api-core, apache-beam, opentelemetry-resourcedetector-gcp, google-genai, google-cloud-core, google-cloud-vision, google-cloud-videointelligence, google-cloud-storage, google-cloud-resource-manager, google-cloud-recommendations-ai, google-cloud-pubsub, google-cloud-monitoring, google-cloud-language, google-cloud-firestore, google-cloud-dlp, google-cloud-datastore, google-cloud-bigtable, google-cloud-bigquery-storage, google-cloud-bigquery, google-cloud-spanner, google-cloud-pubsublite, google-cloud-aiplatform\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145/145\u001b[0m [google-cloud-aiplatform]platform]ery-storage]\n",
      "\u001b[1A\u001b[2KSuccessfully installed PyJWT-2.10.1 absl-py-2.3.1 annotated-types-0.7.0 anyio-4.12.1 apache-beam-2.62.0 astunparse-1.6.3 attrs-25.4.0 cachetools-5.5.2 certifi-2026.1.4 cffi-2.0.0 charset_normalizer-3.4.4 cloudpickle-2.2.1 crcmod-1.7 cryptography-46.0.3 dill-0.3.1.1 distro-1.9.0 dnspython-2.8.0 docopt-0.6.2 docstring_parser-0.17.0 fastavro-1.12.1 fasteners-0.20 filelock-3.20.3 flatbuffers-25.12.19 fsspec-2026.1.0 gast-0.7.0 google-api-core-2.29.0 google-apitools-0.5.31 google-auth-2.48.0rc0 google-auth-httplib2-0.2.1 google-cloud-aiplatform-1.134.0 google-cloud-bigquery-3.40.0 google-cloud-bigquery-storage-2.36.0 google-cloud-bigtable-2.35.0 google-cloud-core-2.5.0 google-cloud-datastore-2.23.0 google-cloud-dlp-3.34.0 google-cloud-firestore-2.23.0 google-cloud-language-2.19.0 google-cloud-monitoring-2.29.0 google-cloud-pubsub-2.34.0 google-cloud-pubsublite-1.13.0 google-cloud-recommendations-ai-0.10.18 google-cloud-resource-manager-1.16.0 google-cloud-spanner-3.62.0 google-cloud-storage-2.19.0 google-cloud-videointelligence-2.18.0 google-cloud-vision-3.12.0 google-crc32c-1.8.0 google-genai-1.59.0 google-pasta-0.2.0 google-resumable-media-2.8.0 googleapis-common-protos-1.72.0 grpc-google-iam-v1-0.14.3 grpc-interceptor-0.15.4 grpcio-1.65.5 grpcio-status-1.65.5 h11-0.16.0 h5py-3.15.1 hdfs-2.7.3 hf-xet-1.2.0 httpcore-1.0.9 httplib2-0.22.0 httpx-0.28.1 huggingface-hub-0.36.0 idna-3.11 importlib-metadata-8.7.1 jaraco.classes-3.4.0 jaraco.context-6.1.0 jaraco.functools-4.4.0 jinja2-3.1.6 jsonpickle-3.4.2 jsonschema-4.26.0 jsonschema-specifications-2025.9.1 keras-3.13.1 keyring-25.7.0 keyrings.google-artifactregistry-auth-1.1.2 libclang-18.1.1 markdown-3.10 markdown-it-py-4.0.0 markupsafe-3.0.3 mdurl-0.1.2 ml-dtypes-0.4.1 mmh3-5.2.0 more-itertools-10.8.0 mpmath-1.3.0 namex-0.1.0 networkx-3.6.1 numpy-2.0.2 oauth2client-4.1.3 objsize-0.7.1 opentelemetry-api-1.39.1 opentelemetry-resourcedetector-gcp-1.11.0a0 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 opt-einsum-3.4.0 optree-0.18.0 orjson-3.11.5 overrides-7.7.0 pandas-2.3.3 pluggy-1.6.0 proto-plus-1.27.0 protobuf-5.29.5 pyarrow-16.1.0 pyarrow-hotfix-0.7 pyasn1-0.6.2 pyasn1-modules-0.4.2 pycparser-3.0 pydantic-2.12.5 pydantic-core-2.41.5 pydot-1.4.2 pymongo-4.16.0 pyparsing-3.3.2 pytz-2025.2 pyyaml-6.0.3 redis-5.3.1 referencing-0.37.0 regex-2026.1.15 requests-2.32.5 rich-14.2.0 rpds-py-0.30.0 rsa-4.9.1 safetensors-0.7.0 sniffio-1.3.1 sortedcontainers-2.4.0 soundfile-0.13.1 sqlparse-0.5.5 sympy-1.13.1 tenacity-9.1.2 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 termcolor-3.3.0 tokenizers-0.20.3 torch-2.6.0 torchaudio-2.6.0 tqdm-4.67.1 transformers-4.45.0 typing-inspection-0.4.2 tzdata-2025.3 urllib3-2.6.3 websockets-15.0.1 werkzeug-3.1.5 wrapt-2.0.1 zipp-3.23.0 zstandard-0.25.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51cfc085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "import logging\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6b48dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logs\n",
    "logging.basicConfig(level=logging.INFO, format=\"edem_apache_beam-%(message)s\")\n",
    "\n",
    "# Set Apache Beam log level to WARNING\n",
    "logging.getLogger('apache_beam').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab083ece",
   "metadata": {},
   "source": [
    "## Apache Beam Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e0c12",
   "metadata": {},
   "source": [
    "Apache Beam is a unified programming model for parallel data processing that provides a rich set of transformations to efficiently manipulate and process both batch and streaming data across multiple execution environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b0f435",
   "metadata": {},
   "source": [
    "### **Apache Beam Core Concepts** \n",
    "\n",
    "##### **Pipeline**\n",
    "A **Pipeline** represents the entire workflow for processing data in Apache Beam. It defines the sequence of transformations applied to input data to produce outputs.\n",
    "\n",
    "##### **PCollection**\n",
    "A **PCollection** is an immutable, distributed dataset that serves as the input and output for transformations in a pipeline. It can handle bounded (batch) or unbounded (streaming) data.\n",
    "\n",
    "##### **Transformations**\n",
    "A **Transformation** is an operation applied to a PCollection to produce one or more output PCollections. Common transformations include Map, FlatMap, GroupByKey, and Combine.\n",
    "\n",
    "##### **PTransform**\n",
    "A **PTransform** is a reusable and composable abstraction that encapsulates one or more transformations. It simplifies pipelines by modularizing complex workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aac87ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edem_apache_beam-Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edem_apache_beam-Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edem_apache_beam-Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edem_apache_beam-1\n",
      "edem_apache_beam-2\n",
      "edem_apache_beam-3\n",
      "edem_apache_beam-4\n",
      "edem_apache_beam-5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exercise 01: What is a PCollection?\n",
    "\n",
    "PCollection: A distributed dataset in Apache Beam\n",
    "\"\"\"\n",
    "\n",
    "#  Create and explore a simple PCollection\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "    \n",
    "    # Create a PCollection\n",
    "    numbers = pipeline | \"Create Numbers\" >> beam.Create([1, 2, 3, 4, 5])\n",
    "\n",
    "    # Log output\n",
    "    numbers | \"Log Elements\" >> beam.Map(logging.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42738f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edem_apache_beam-1\n",
      "edem_apache_beam-4\n",
      "edem_apache_beam-9\n",
      "edem_apache_beam-16\n",
      "edem_apache_beam-25\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exercise 02: Declaring a Pipeline\n",
    "\n",
    "Pipeline: Defines the workflow of data transformations.\n",
    "\"\"\"\n",
    "\n",
    "# Interactive pipeline\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "    \n",
    "    # Create a PCollection and perform a transformation\n",
    "    squared_numbers = (\n",
    "        pipeline\n",
    "        | \"Create Numbers\" >> beam.Create([1, 2, 3, 4, 5])\n",
    "        | \"Square Numbers\" >> beam.Map(lambda x: x * x)\n",
    "    )\n",
    "\n",
    "    # Log output\n",
    "    squared_numbers | \"Log Squared Numbers\" >> beam.Map(logging.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21c88277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edem_apache_beam-('En', 1)\n",
      "edem_apache_beam-('un', 2)\n",
      "edem_apache_beam-('lugar', 1)\n",
      "edem_apache_beam-('de', 12)\n",
      "edem_apache_beam-('la', 1)\n",
      "edem_apache_beam-('Mancha,', 1)\n",
      "edem_apache_beam-('cuyo', 1)\n",
      "edem_apache_beam-('nombre', 1)\n",
      "edem_apache_beam-('no', 2)\n",
      "edem_apache_beam-('quiero', 1)\n",
      "edem_apache_beam-('acordarme,', 1)\n",
      "edem_apache_beam-('ha', 1)\n",
      "edem_apache_beam-('mucho', 1)\n",
      "edem_apache_beam-('tiempo', 1)\n",
      "edem_apache_beam-('que', 2)\n",
      "edem_apache_beam-('vivía', 1)\n",
      "edem_apache_beam-('hidalgo', 1)\n",
      "edem_apache_beam-('los', 5)\n",
      "edem_apache_beam-('lanza', 1)\n",
      "edem_apache_beam-('en', 1)\n",
      "edem_apache_beam-('astillero,', 1)\n",
      "edem_apache_beam-('adarga', 1)\n",
      "edem_apache_beam-('antigua,', 1)\n",
      "edem_apache_beam-('rocín', 1)\n",
      "edem_apache_beam-('flaco', 1)\n",
      "edem_apache_beam-('y', 2)\n",
      "edem_apache_beam-('galgo', 1)\n",
      "edem_apache_beam-('corredor.', 1)\n",
      "edem_apache_beam-('Una', 1)\n",
      "edem_apache_beam-('olla', 1)\n",
      "edem_apache_beam-('algo', 1)\n",
      "edem_apache_beam-('más', 3)\n",
      "edem_apache_beam-('vaca', 1)\n",
      "edem_apache_beam-('carnero,', 1)\n",
      "edem_apache_beam-('salpicón', 1)\n",
      "edem_apache_beam-('las', 3)\n",
      "edem_apache_beam-('noches,', 1)\n",
      "edem_apache_beam-('duelos', 1)\n",
      "edem_apache_beam-('quebrantos', 1)\n",
      "edem_apache_beam-('sábados,', 1)\n",
      "edem_apache_beam-('lantejas', 1)\n",
      "edem_apache_beam-('viernes,', 1)\n",
      "edem_apache_beam-('algún', 1)\n",
      "edem_apache_beam-('palomino', 1)\n",
      "edem_apache_beam-('añadidura', 1)\n",
      "edem_apache_beam-('domingos,', 1)\n",
      "edem_apache_beam-('consumían', 1)\n",
      "edem_apache_beam-('tres', 1)\n",
      "edem_apache_beam-('partes', 1)\n",
      "edem_apache_beam-('su', 2)\n",
      "edem_apache_beam-('hacienda.', 1)\n",
      "edem_apache_beam-('El', 1)\n",
      "edem_apache_beam-('resto', 1)\n",
      "edem_apache_beam-('della', 1)\n",
      "edem_apache_beam-('concluían', 1)\n",
      "edem_apache_beam-('sayo', 1)\n",
      "edem_apache_beam-('velarte,', 1)\n",
      "edem_apache_beam-('calzas', 1)\n",
      "edem_apache_beam-('velludo', 1)\n",
      "edem_apache_beam-('para', 1)\n",
      "edem_apache_beam-('fiestas', 1)\n",
      "edem_apache_beam-('con', 2)\n",
      "edem_apache_beam-('sus', 1)\n",
      "edem_apache_beam-('pantuflos', 1)\n",
      "edem_apache_beam-('lo', 2)\n",
      "edem_apache_beam-('mismo,', 1)\n",
      "edem_apache_beam-('días', 1)\n",
      "edem_apache_beam-('entre', 1)\n",
      "edem_apache_beam-('semana', 1)\n",
      "edem_apache_beam-('se', 1)\n",
      "edem_apache_beam-('honraba', 1)\n",
      "edem_apache_beam-('vellorí', 1)\n",
      "edem_apache_beam-('fino.', 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exercise 03: Transformations\n",
    "\n",
    "Transforms one or more input PCollections into one or more output PCollections.\n",
    "\"\"\"\n",
    "\n",
    "# Interactive pipeline\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            # Input\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText(\n",
    "                './00_DocAux/input_text.txt')\n",
    "            # Transformations\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"Map\" >> beam.Map(lambda x: (x,1))\n",
    "            | \"Combine\" >> beam.CombinePerKey(sum)\n",
    "            # Output\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13087301",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 04: Reuse Transformations (PTransforms)\n",
    "\n",
    "PTransform: A collection of transformations packaged as a reusable unit for complex workflows.\n",
    "\"\"\"\n",
    "\n",
    "class MyCustomTransform(beam.PTransform):\n",
    "    \n",
    "    def expand(self, pcoll):\n",
    "        return (\n",
    "            pcoll\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"Map\" >> beam.Map(lambda x: (x,1))\n",
    "            | \"Combine\" >> beam.CombinePerKey(sum)\n",
    "        )\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./00_DocAux/input_text.txt')\n",
    "            | \"Apply Custom Transform\" >> MyCustomTransform()\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32529521",
   "metadata": {},
   "source": [
    "#### **Apache Beam Transformations** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378df144",
   "metadata": {},
   "source": [
    "**A. GroupByKey**\n",
    "\n",
    "It groups the elements of a data bundle by a common key, producing a collection where each key is unique and associated with a list of values that share that key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9b9d9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "edem_apache_beam-('Spain', ['Valencia', 'Barcelona'])\n",
      "edem_apache_beam-('France', ['Paris'])\n"
     ]
    }
   ],
   "source": [
    "\"\"\" GroupByKey \"\"\"\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    data = (p | \"PCollection\" >> beam.Create([('Spain', 'Valencia'), ('Spain','Barcelona'), ('France', 'Paris')]))\n",
    "\n",
    "    (data \n",
    "        | \"Combined\" >> beam.GroupByKey()\n",
    "        | \"Print\" >> beam.Map(logging.info))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4d4b26",
   "metadata": {},
   "source": [
    "**B. CoGroupByKey** \n",
    "\n",
    "It merges two PCollections by key, producing pairs where each key is associated with lists of elements from both input collections. This is useful for operations involving data from two different sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55604616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def fun(element):\n",
    "    country,city = element\n",
    "    if country == 'Spain' :\n",
    "        for city in country:\n",
    "            if city == 'Valencia':\n",
    "                return city\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    p1 = p | \"PCollection 01\" >> beam.Create([('Spain', 'Valencia'), ('Spain','Barcelona'), ('France', 'Paris')])\n",
    "    p2 = p | \"PCollection 02\" >> beam.Create([('Spain', 'Madrid'), ('Spain','Alicante'), ('France', 'Lyon')])\n",
    "\n",
    "    data = ((p1,p2) | beam.CoGroupByKey())\n",
    "    bestcity = data | beam.Map(fun)\n",
    "    bestcity | beam.Map(print)\n",
    "\n",
    "    # data | \"Print\" >> beam.Map(logging.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e798a5",
   "metadata": {},
   "source": [
    "**C. Flatten**  \n",
    "\n",
    "It merges multiple PCollections into a single PCollection, combining their elements into one flat structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd9f59e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madrid\n",
      "Barcelona\n",
      "Valencia\n",
      "Malaga\n",
      "New York\n",
      "Los Angeles\n",
      "Miami\n",
      "Chicago\n",
      "London\n",
      "Manchester\n",
      "Liverpool\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    p1 = p | \"PCollection 01\" >> beam.Create(['New York', 'Los Angeles', 'Miami', 'Chicago'])\n",
    "    p2 = p | \"Pcollection 02\" >> beam.Create(['Madrid', 'Barcelona', 'Valencia', 'Malaga'])\n",
    "    p3 = p | \"Pcollection 03\" >> beam.Create(['London','Manchester', 'Liverpool'])\n",
    "\n",
    "    merged = ((p1,p2,p3)| beam.Flatten())\n",
    "\n",
    "    merged | beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7986675c",
   "metadata": {},
   "source": [
    "**D. Partition**  \n",
    "\n",
    "It splits a PCollection into multiple partitions based on defined criteria, enabling parallel and distributed processing of subsets of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8448bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['Spain', 'USA', 'Switzerland']\n",
    "\n",
    "def choose_partition(city, num_partitions):\n",
    "    \n",
    "    usa = {'New York', 'Los Angeles', 'Miami', 'Chicago'}\n",
    "    eu  = {'Madrid', 'Barcelona', 'Valencia', 'Malaga'}\n",
    "    uk  = {'London', 'Manchester', 'Liverpool'}\n",
    "\n",
    "    if city in usa:\n",
    "        return 0  \n",
    "    if city in eu:\n",
    "        return 1  \n",
    "    if city in uk:\n",
    "        return 2 \n",
    "    return 3 \n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "        usa,eu,uk,others = (\n",
    "                p \n",
    "                | \"PCollection\" >> beam.Create([\n",
    "                        'New York', 'Los Angeles', 'Miami', 'Chicago',\n",
    "                        'Madrid', 'Barcelona', 'Valencia', 'Malaga',\n",
    "                        'London', 'Manchester', 'Liverpool','Tokyo' \n",
    "                ])\n",
    "                | \"partition\" >> beam.Partition(choose_partition, 4)\n",
    "        )\n",
    "\n",
    "        eu |  beam.Map(logging.info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34356117",
   "metadata": {},
   "source": [
    "## Introducing complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1fc54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 05: This exercise shows how to achieve the same transformation\n",
    "    using both Map and ParDo with DoFn.\n",
    "\n",
    "Use Map for simple transformations where the logic is inline and does not require lifecycle methods.\n",
    "Use ParDo with DoFn for more complex operations requiring setup, cleanup, or state management.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Map \"\"\"\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./00_DocAux/input_text.txt')\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"Map\" >> beam.Map(lambda x: x.upper())\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39217964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ParDo with DoFn\"\"\"\n",
    "\n",
    "class UpperCaseDoFn(beam.DoFn):\n",
    "\n",
    "    def setup(self):\n",
    "        logging.info(\"Loading model...\")\n",
    "        self.model = lambda x: x.upper()\n",
    "\n",
    "    def process(self, element):\n",
    "        yield self.model(element)\n",
    "\n",
    "    def teardown(self):\n",
    "        logging.info(\"Releasing model resources.\")\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./00_DocAux/input_text.txt')\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"DoFn\" >> beam.ParDo(UpperCaseDoFn())\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d7f06",
   "metadata": {},
   "source": [
    "### **Exploring the DoFn LifeCycle**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92b92b2",
   "metadata": {},
   "source": [
    "The Lifecycle of a **DoFn** in Apache Beam refers to the stages that an instance of the `DoFn` class goes through from initialization to completion within the context of a *ParDo* transformation. Below are the key phases of the DoFn life cycle:\n",
    "\n",
    "1. **Setup (Initialization):**  \n",
    "   - **Goal:** This phase occurs once for each instance of `DoFn` before processing begins.  \n",
    "   - **Process:** Initialization tasks, such as allocating resources, establishing connections, or loading models, are performed in the `setup(self)` method.\n",
    "\n",
    "2. **Processing Elements (Process):**  \n",
    "   - **Goal:** This phase executes the main logic for each input element in the data bundle.  \n",
    "   - **Process:** The core processing logic is implemented in the `process(self, element)` method. This method is called for each element in the input `PCollection` and defines how the data is transformed.\n",
    "\n",
    "3. **Start Bundle and Finish Bundle:**  \n",
    "   - **Goal:** These phases occur once before (`start_bundle(self)`) and after (`finish_bundle(self)`) processing all elements in a bundle. A bundle represents a chunk of data processed in parallel.  \n",
    "   - **Process:** Bundle-specific setup and cleanup tasks, such as initializing or finalizing temporary states or resources used within the bundle, are performed here.\n",
    "\n",
    "4. **Teardown (Finalization):**  \n",
    "   - **Goal:** This phase occurs once after all elements have been processed.  \n",
    "   - **Process:** The `teardown(self)` method handles final resource cleanup, such as closing database connections, releasing memory, or terminating external processes.\n",
    "\n",
    "---\n",
    "\n",
    "The lifecycle of a **DoFn** provides a structured framework for managing resources and processing logic in *ParDo* transformations. Each instance is created, initialized, processes elements, and performs cleanup in a controlled and efficient manner. This ensures proper resource management and facilitates robust, scalable data processing pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528ec3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise: Understanding the lifecycle methods (setup, start_bundle, process, finish_bundle, and teardown)\n",
    " and how they are invoked during pipeline execution.\n",
    "\n",
    "Use ParDo with DoFn for tasks requiring state, lifecycle, or multiple outputs.\n",
    "\"\"\"\n",
    "\n",
    "class LifecycleDoFn(beam.DoFn):\n",
    "\n",
    "    def setup(self):\n",
    "        logging.info(\"Setting up resources.\")\n",
    "\n",
    "    def start_bundle(self):\n",
    "        logging.info(\"Starting a bundle.\")\n",
    "\n",
    "    def process(self, element):\n",
    "        logging.info(f\"Processing element: {element}\")\n",
    "        yield element.upper()\n",
    "\n",
    "    def finish_bundle(self):\n",
    "        logging.info(\"Finishing a bundle.\")\n",
    "\n",
    "    def teardown(self):\n",
    "        logging.info(\"Tearing down resources.\")\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./00_DocAux/input_text.txt')\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"Apply Lifecycle DoFn\" >> beam.ParDo(LifecycleDoFn())\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3b32a9",
   "metadata": {},
   "source": [
    "### **DoFn: Stateful Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda7b7e",
   "metadata": {},
   "source": [
    "- Allows operations that depend on previous or cumulative data.\n",
    "- Reduces the need to rely on external systems, such as databases or caching systems.\n",
    "- Efficient, as the state is stored locally for each key and window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32adfaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.transforms.userstate import CombiningValueStateSpec\n",
    "\n",
    "\"\"\"\n",
    "Exercise: The state is persisted across multiple calls to process()\n",
    "\"\"\"\n",
    "\n",
    "class StatefulDoFn(beam.DoFn):\n",
    "    \n",
    "    # Define a state spec to maintain counts\n",
    "    state_spec = CombiningValueStateSpec(\"count\", sum)\n",
    "\n",
    "    def process(self, element, state=beam.DoFn.StateParam(state_spec)):\n",
    "        k,v = element\n",
    "        state.add(1)\n",
    "        yield f\"Key: {k}, Count: {state.read()}\"\n",
    "    \n",
    "# Interactive pipeline\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./00_DocAux/input_text.txt')\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"Map\" >> beam.Map(lambda x: (x.lower(),1))\n",
    "            | \"Count Events Per Key\" >> beam.ParDo(StatefulDoFn())\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df0cb7",
   "metadata": {},
   "source": [
    "### Beam ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b3c094",
   "metadata": {},
   "source": [
    "- The **RunInference** transform enables the execution of machine learning inference directly within an Apache Beam pipeline by applying a preconfigured model to incoming data elements\n",
    "\n",
    "- **MLTransform** is used within a pipeline to prepare data for machine learning model training. It encapsulates multiple data preprocessing operations into a single abstraction, enabling the application of diverse transformation workflows through a unified interfac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd8ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch --quiet\n",
    "!pip install tensorflow --quiet\n",
    "!pip install transformers==4.44.2 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f277aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from typing import Iterable\n",
    "from typing import Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForMaskedLM\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.ml.inference.base import KeyedModelHandler\n",
    "from apache_beam.ml.inference.base import PredictionResult\n",
    "from apache_beam.ml.inference.base import RunInference\n",
    "from apache_beam.ml.inference.huggingface_inference import HuggingFacePipelineModelHandler\n",
    "from apache_beam.ml.inference.huggingface_inference import HuggingFaceModelHandlerKeyedTensor\n",
    "from apache_beam.ml.inference.huggingface_inference import HuggingFaceModelHandlerTensor\n",
    "from apache_beam.ml.inference.huggingface_inference import PipelineTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389cb5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_handler = HuggingFacePipelineModelHandler(\n",
    "    task=PipelineTask.Translation_XX_to_YY,\n",
    "    model = \"google/flan-t5-small\",\n",
    "    load_pipeline_args={'framework': 'pt', \"device\": -1},\n",
    "    inference_args={'max_new_tokens': 200}\n",
    ")\n",
    "\n",
    "class FormatOutput(beam.DoFn):\n",
    "    def process(self, element: PredictionResult[Dict]) -> Iterable[Tuple[str, str]]:\n",
    "        \n",
    "        translated_text = element.inference[0]['translation_text']\n",
    "\n",
    "        logging.info(f\"Translated Text: {translated_text}\")\n",
    "        \n",
    "        yield translated_text\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./00_DocAux/input_text.txt')\n",
    "            | \"RunInference\" >> RunInference(model_handler)\n",
    "            | \"Print\" >> beam.ParDo(FormatOutput())\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d6d503",
   "metadata": {},
   "source": [
    "### **Streaming** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2afee27",
   "metadata": {},
   "source": [
    "#### GCP Setup\n",
    "\n",
    "- PubSub Topics\n",
    "\n",
    "```\n",
    "gcloud pubsub topics create <TOPIC_NAME>\n",
    "```\n",
    "\n",
    "- PubSub Subscriptions\n",
    "\n",
    "```\n",
    "gcloud pubsub subscriptions create <SUBSCRIPTION_NAME> --topic=<TOPIC_NAME>\n",
    "```\n",
    "\n",
    "- Google Cloud Storage Bucket  \n",
    "\n",
    "```\n",
    "gcloud storage mb gs://<BUCKET_NAME> --location=<REGION_ID>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da1a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "project_id = \"\"\n",
    "subscription_name = \"\"\n",
    "bq_dataset = \"\"\n",
    "bq_table = \"\"\n",
    "bucket_name = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbae5c76",
   "metadata": {},
   "source": [
    "**Window in Apache Beam**\n",
    "  \n",
    "A window in Apache Beam defines a time frame for **organizing and grouping data elements** during processing, enabling time-based operations.\n",
    "\n",
    "---\n",
    "\n",
    "**Types of Windows:**\n",
    "\n",
    "- **Fixed Window:** \n",
    "\n",
    "  Divides elements into fixed time intervals, segmenting the PCollection into evenly spaced, time-based windows.\n",
    "\n",
    "- **Sliding Window:**  \n",
    "\n",
    "  Creates overlapping windows with a specified size and stride, enabling continuous analysis of data over time.\n",
    "\n",
    "- **Session Window:**  \n",
    "\n",
    "  Groups elements based on **contiguous temporal activity**, where windows are dynamically defined by an **inactivity gap** between events, making it ideal for capturing logical sessions in streaming data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911fece6",
   "metadata": {},
   "source": [
    "**A. FixedWindows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3af993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.transforms.window import FixedWindows\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "    events = [\n",
    "        beam.window.TimestampedValue(\"event1\", 60),   \n",
    "        beam.window.TimestampedValue(\"event2\", 180),  \n",
    "        beam.window.TimestampedValue(\"event3\", 360), \n",
    "    ]\n",
    "\n",
    "    (\n",
    "        p\n",
    "        | \"Create PCollection\" >> beam.Create(events)\n",
    "        | \"Sliding Window\" >> beam.WindowInto(FixedWindows(size=300))\n",
    "        | \"k,v\" >> beam.Map(lambda x: (\"key\", x))\n",
    "        | \"Combine\" >> beam.GroupByKey()\n",
    "        | \"Print\" >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2087a3ad",
   "metadata": {},
   "source": [
    "**B. SlidingWindows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed0785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.transforms.window import SlidingWindows\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "    events = [\n",
    "        beam.window.TimestampedValue(\"event1\", 60),   \n",
    "        beam.window.TimestampedValue(\"event2\", 180),  \n",
    "        beam.window.TimestampedValue(\"event3\", 360), \n",
    "    ]\n",
    "\n",
    "    (\n",
    "        p\n",
    "        | \"Create PCollection\" >> beam.Create(events)\n",
    "        | \"Sliding Window\" >> beam.WindowInto(SlidingWindows(size=300, period=120))\n",
    "        | \"k,v\" >> beam.Map(lambda x: (\"key\", x))\n",
    "        | \"Combine\" >> beam.GroupByKey()\n",
    "        | \"Print\" >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be21097f",
   "metadata": {},
   "source": [
    "**C. SessionWindows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf567b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.transforms.window import Sessions\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "    events = [\n",
    "        beam.window.TimestampedValue(\"event1\", 0),   \n",
    "        beam.window.TimestampedValue(\"event2\", 5),  \n",
    "        beam.window.TimestampedValue(\"event3\", 25),\n",
    "        beam.window.TimestampedValue(\"event4\", 27), \n",
    "    ]\n",
    "\n",
    "    (\n",
    "        p\n",
    "        | \"Create PCollection\" >> beam.Create(events)\n",
    "        | \"Sliding Window\" >> beam.WindowInto(Sessions(gap_size=7))\n",
    "        | \"k,v\" >> beam.Map(lambda x: (\"key\", x))\n",
    "        | \"Combine\" >> beam.GroupByKey()\n",
    "        | \"Print\" >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874fa2e3",
   "metadata": {},
   "source": [
    "**D. Triggers, Watermark & Allowed Lateness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3736151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.testing.test_stream import TestStream\n",
    "from apache_beam.transforms.window import FixedWindows\n",
    "from apache_beam.transforms.trigger import AfterWatermark, AfterProcessingTime\n",
    "from apache_beam.transforms.trigger import AccumulationMode\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "opts = PipelineOptions(flags=[\"--allow_unsafe_triggers\"],streaming=True, runner=\"DirectRunner\")\n",
    "\n",
    "with beam.Pipeline(options=opts) as p:\n",
    "    events = (\n",
    "        TestStream()\n",
    "        .add_elements([\n",
    "            beam.window.TimestampedValue(\"event1\", 0),\n",
    "            beam.window.TimestampedValue(\"event2\", 2),\n",
    "        ])\n",
    "        .advance_processing_time(5)\n",
    "        .add_elements([\n",
    "            beam.window.TimestampedValue(\"event3\", 4),\n",
    "        ])\n",
    "        .advance_watermark_to(10)\n",
    "        .add_elements([\n",
    "            beam.window.TimestampedValue(\"late_event\", 1),\n",
    "        ])\n",
    "        .advance_watermark_to_infinity()\n",
    "    )\n",
    "\n",
    "    (\n",
    "        p\n",
    "        | \"Stream\" >> events\n",
    "        | \"Window\" >> beam.WindowInto(\n",
    "            FixedWindows(5),\n",
    "            trigger=AfterWatermark(early=AfterProcessingTime(3)),\n",
    "            allowed_lateness=10,\n",
    "            accumulation_mode=AccumulationMode.ACCUMULATING\n",
    "        )\n",
    "        | \"k,v\" >> beam.Map(lambda x: (\"key\", x))\n",
    "        | \"Group\" >> beam.GroupByKey()\n",
    "        | \"Print\" >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8161b20",
   "metadata": {},
   "source": [
    "### Apache Beam on Google Cloud: Dataflow Pipeline from Pub/Sub to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b5418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import json\n",
    "import logging\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "def decode_message(msg):\n",
    "\n",
    "    output = msg.decode('utf-8')\n",
    "    logging.info(\"New PubSub Message: %s\", output)\n",
    "\n",
    "    return json.loads(output)\n",
    "\n",
    "def run():\n",
    "    with beam.Pipeline(options=PipelineOptions(\n",
    "        streaming=True,\n",
    "        save_main_session=True,\n",
    "        job_name = \"edem-pubsub-to-bigquery\",\n",
    "        project=project_id,\n",
    "        runner=\"DataflowRunner\",\n",
    "        temp_location=f\"gs://{bucket_name}/tmp\",\n",
    "        staging_location=f\"gs://{bucket_name}/staging\",\n",
    "        region=\"europe-southwest1\"\n",
    "    )) as p:\n",
    "        \n",
    "        (\n",
    "            p \n",
    "            | \"ReadFromPubSub\" >> beam.io.ReadFromPubSub(subscription=f'projects/{project_id}/subscriptions/{subscription_name}')\n",
    "            | \"decode msg\" >> beam.Map(decode_message)\n",
    "            | \"Write to BigQuery\" >> beam.io.WriteToBigQuery(\n",
    "                table = f\"{project_id}:{bq_dataset}.{bq_table}\", # Required Format: PROJECT_ID:DATASET.TABLE\n",
    "                schema='name:STRING', # Required Format: field_name:TYPE\n",
    "                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Run Process\n",
    "logging.info(\"The process started\")\n",
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_def",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
