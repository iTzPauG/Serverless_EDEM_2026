{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c397ee4",
   "metadata": {},
   "source": [
    "## **Apache Beam Basics**\n",
    "\n",
    "Description: This notebook will teach you the basics of the Apache Beam Programming Model:\n",
    "\n",
    "- Apache Beam Basics: Pipeline, PCollection, Transforms & PTransforms.\n",
    "   - GroupByKey\n",
    "   - CoGroupByKey\n",
    "   - Flatten\n",
    "   - Partition\n",
    "- Introducing complexity: DoFn.\n",
    "   - DoFn vs Map\n",
    "   - DoFn LifeCycle\n",
    "   - DoFn Stateful Processing\n",
    "- Advanced: Streaming.\n",
    "   - Fixed Windows\n",
    "   - Sliding Windows\n",
    "   - PubSub to Bigquery\n",
    "\n",
    "\n",
    "EDEM. Master Big Data & Cloud 2025/2026<br>\n",
    "Professor: Javi Briones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d25925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"apache-beam[gcp]\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cfc085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b48dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logs\n",
    "logging.basicConfig(level=logging.INFO, format=\"edem_apache_beam-%(message)s\")\n",
    "\n",
    "# Set Apache Beam log level to WARNING\n",
    "logging.getLogger('apache_beam').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab083ece",
   "metadata": {},
   "source": [
    "## Apache Beam Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e0c12",
   "metadata": {},
   "source": [
    "Apache Beam is a unified programming model for parallel data processing that provides a rich set of transformations to efficiently manipulate and process both batch and streaming data across multiple execution environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b0f435",
   "metadata": {},
   "source": [
    "### **Apache Beam Core Concepts** \n",
    "\n",
    "##### **Pipeline**\n",
    "A **Pipeline** represents the entire workflow for processing data in Apache Beam. It defines the sequence of transformations applied to input data to produce outputs.\n",
    "\n",
    "##### **PCollection**\n",
    "A **PCollection** is an immutable, distributed dataset that serves as the input and output for transformations in a pipeline. It can handle bounded (batch) or unbounded (streaming) data.\n",
    "\n",
    "##### **Transformations**\n",
    "A **Transformation** is an operation applied to a PCollection to produce one or more output PCollections. Common transformations include Map, FlatMap, GroupByKey, and Combine.\n",
    "\n",
    "##### **PTransform**\n",
    "A **PTransform** is a reusable and composable abstraction that encapsulates one or more transformations. It simplifies pipelines by modularizing complex workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aac87ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 01: What is a PCollection?\n",
    "\n",
    "PCollection: A distributed dataset in Apache Beam\n",
    "\"\"\"\n",
    "\n",
    "#  Create and explore a simple PCollection\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "    \n",
    "    # Create a PCollection\n",
    "    numbers = pipeline | \"Create Numbers\" >> beam.Create([1, 2, 3, 4, 5])\n",
    "\n",
    "    # Log output\n",
    "    numbers | \"Log Elements\" >> beam.Map(logging.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42738f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 02: Declaring a Pipeline\n",
    "\n",
    "Pipeline: Defines the workflow of data transformations.\n",
    "\"\"\"\n",
    "\n",
    "# Interactive pipeline\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "    \n",
    "    # Create a PCollection and perform a transformation\n",
    "    squared_numbers = (\n",
    "        pipeline\n",
    "        | \"Create Numbers\" >> beam.Create([1, 2, 3, 4, 5])\n",
    "        | \"Square Numbers\" >> beam.Map(lambda x: x * x)\n",
    "    )\n",
    "\n",
    "    # Log output\n",
    "    squared_numbers | \"Log Squared Numbers\" >> beam.Map(logging.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c88277",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 03: Transformations\n",
    "\n",
    "Transforms one or more input PCollections into one or more output PCollections.\n",
    "\"\"\"\n",
    "\n",
    "# Interactive pipeline\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            # Input\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText(\n",
    "                './00_DocAux/input_text.txt')\n",
    "            # Transformations\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"Map\" >> beam.Map(lambda x: (x,1))\n",
    "            | \"Combine\" >> beam.CombinePerKey(sum)\n",
    "            # Output\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13087301",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 04: Reuse Transformations (PTransforms)\n",
    "\n",
    "PTransform: A collection of transformations packaged as a reusable unit for complex workflows.\n",
    "\"\"\"\n",
    "\n",
    "class MyCustomTransform(beam.PTransform):\n",
    "    \n",
    "    def expand(self, pcoll):\n",
    "        return (\n",
    "            pcoll\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"Map\" >> beam.Map(lambda x: (x,1))\n",
    "            | \"Combine\" >> beam.CombinePerKey(sum)\n",
    "        )\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./00_DocAux/input_text.txt')\n",
    "            | \"Apply Custom Transform\" >> MyCustomTransform()\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32529521",
   "metadata": {},
   "source": [
    "#### **Apache Beam Transformations** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378df144",
   "metadata": {},
   "source": [
    "**A. GroupByKey**\n",
    "\n",
    "It groups the elements of a data bundle by a common key, producing a collection where each key is unique and associated with a list of values that share that key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b9d9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" GroupByKey \"\"\"\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    data = (p | \"PCollection\" >> beam.Create([('Spain', 'Valencia'), ('Spain','Barcelona'), ('France', 'Paris')]))\n",
    "\n",
    "    (data \n",
    "        | \"Combined\" >> beam.GroupByKey()\n",
    "        | \"Print\" >> beam.Map(logging.info))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4d4b26",
   "metadata": {},
   "source": [
    "**B. CoGroupByKey** \n",
    "\n",
    "It merges two PCollections by key, producing pairs where each key is associated with lists of elements from both input collections. This is useful for operations involving data from two different sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55604616",
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    p1 = p | \"PCollection 01\" >> beam.Create([('Spain', 'Valencia'), ('Spain','Barcelona'), ('France', 'Paris')])\n",
    "    p2 = p | \"PCollection 02\" >> beam.Create([('Spain', 'Madrid'), ('Spain','Alicante'), ('France', 'Lyon')])\n",
    "\n",
    "    data = ((p1,p2) | beam.CoGroupByKey())\n",
    "\n",
    "    data | \"Print\" >> beam.Map(logging.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e798a5",
   "metadata": {},
   "source": [
    "**C. Flatten**  \n",
    "\n",
    "It merges multiple PCollections into a single PCollection, combining their elements into one flat structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9f59e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    p1 = p | \"PCollection 01\" >> beam.Create(['New York', 'Los Angeles', 'Miami', 'Chicago'])\n",
    "    p2 = p | \"Pcollection 02\" >> beam.Create(['Madrid', 'Barcelona', 'Valencia', 'Malaga'])\n",
    "    p3 = p | \"Pcollection 03\" >> beam.Create(['London','Manchester', 'Liverpool'])\n",
    "\n",
    "    merged = ((p1,p2,p3)| beam.Flatten())\n",
    "\n",
    "    merged | beam.Map(logging.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7986675c",
   "metadata": {},
   "source": [
    "**D. Partition**  \n",
    "\n",
    "It splits a PCollection into multiple partitions based on defined criteria, enabling parallel and distributed processing of subsets of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8448bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['Spain', 'USA', 'Switzerland']\n",
    "\n",
    "def choose_partition(city, num_partitions):\n",
    "    \n",
    "    usa = {'New York', 'Los Angeles', 'Miami', 'Chicago'}\n",
    "    eu  = {'Madrid', 'Barcelona', 'Valencia', 'Malaga'}\n",
    "    uk  = {'London', 'Manchester', 'Liverpool'}\n",
    "\n",
    "    if city in usa:\n",
    "        return 0  \n",
    "    if city in eu:\n",
    "        return 1  \n",
    "    if city in uk:\n",
    "        return 2 \n",
    "    return 3 \n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "        usa,eu,uk,others = (\n",
    "                p \n",
    "                | \"PCollection\" >> beam.Create([\n",
    "                        'New York', 'Los Angeles', 'Miami', 'Chicago',\n",
    "                        'Madrid', 'Barcelona', 'Valencia', 'Malaga',\n",
    "                        'London', 'Manchester', 'Liverpool','Tokyo' \n",
    "                ])\n",
    "                | \"partition\" >> beam.Partition(choose_partition, 4)\n",
    "        )\n",
    "\n",
    "        eu |  beam.Map(logging.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34356117",
   "metadata": {},
   "source": [
    "## Introducing complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1fc54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 05: This exercise shows how to achieve the same transformation\n",
    "    using both Map and ParDo with DoFn.\n",
    "\n",
    "Use Map for simple transformations where the logic is inline and does not require lifecycle methods.\n",
    "Use ParDo with DoFn for more complex operations requiring setup, cleanup, or state management.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Map \"\"\"\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./00_DocAux/input_text.txt')\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"Map\" >> beam.Map(lambda x: x.upper())\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39217964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ParDo with DoFn\"\"\"\n",
    "\n",
    "class UpperCaseDoFn(beam.DoFn):\n",
    "\n",
    "    def setup(self):\n",
    "        logging.info(\"Loading model...\")\n",
    "        self.model = lambda x: x.upper()\n",
    "\n",
    "    def process(self, element):\n",
    "        yield self.model(element)\n",
    "\n",
    "    def teardown(self):\n",
    "        logging.info(\"Releasing model resources.\")\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./00_DocAux/input_text.txt')\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"DoFn\" >> beam.ParDo(UpperCaseDoFn())\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d7f06",
   "metadata": {},
   "source": [
    "### **Exploring the DoFn LifeCycle**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92b92b2",
   "metadata": {},
   "source": [
    "The Lifecycle of a **DoFn** in Apache Beam refers to the stages that an instance of the `DoFn` class goes through from initialization to completion within the context of a *ParDo* transformation. Below are the key phases of the DoFn life cycle:\n",
    "\n",
    "1. **Setup (Initialization):**  \n",
    "   - **Goal:** This phase occurs once for each instance of `DoFn` before processing begins.  \n",
    "   - **Process:** Initialization tasks, such as allocating resources, establishing connections, or loading models, are performed in the `setup(self)` method.\n",
    "\n",
    "2. **Processing Elements (Process):**  \n",
    "   - **Goal:** This phase executes the main logic for each input element in the data bundle.  \n",
    "   - **Process:** The core processing logic is implemented in the `process(self, element)` method. This method is called for each element in the input `PCollection` and defines how the data is transformed.\n",
    "\n",
    "3. **Start Bundle and Finish Bundle:**  \n",
    "   - **Goal:** These phases occur once before (`start_bundle(self)`) and after (`finish_bundle(self)`) processing all elements in a bundle. A bundle represents a chunk of data processed in parallel.  \n",
    "   - **Process:** Bundle-specific setup and cleanup tasks, such as initializing or finalizing temporary states or resources used within the bundle, are performed here.\n",
    "\n",
    "4. **Teardown (Finalization):**  \n",
    "   - **Goal:** This phase occurs once after all elements have been processed.  \n",
    "   - **Process:** The `teardown(self)` method handles final resource cleanup, such as closing database connections, releasing memory, or terminating external processes.\n",
    "\n",
    "---\n",
    "\n",
    "The lifecycle of a **DoFn** provides a structured framework for managing resources and processing logic in *ParDo* transformations. Each instance is created, initialized, processes elements, and performs cleanup in a controlled and efficient manner. This ensures proper resource management and facilitates robust, scalable data processing pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528ec3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise: Understanding the lifecycle methods (setup, start_bundle, process, finish_bundle, and teardown)\n",
    " and how they are invoked during pipeline execution.\n",
    "\n",
    "Use ParDo with DoFn for tasks requiring state, lifecycle, or multiple outputs.\n",
    "\"\"\"\n",
    "\n",
    "class LifecycleDoFn(beam.DoFn):\n",
    "\n",
    "    def setup(self):\n",
    "        logging.info(\"Setting up resources.\")\n",
    "\n",
    "    def start_bundle(self):\n",
    "        logging.info(\"Starting a bundle.\")\n",
    "\n",
    "    def process(self, element):\n",
    "        logging.info(f\"Processing element: {element}\")\n",
    "        yield element.upper()\n",
    "\n",
    "    def finish_bundle(self):\n",
    "        logging.info(\"Finishing a bundle.\")\n",
    "\n",
    "    def teardown(self):\n",
    "        logging.info(\"Tearing down resources.\")\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./00_DocAux/input_text.txt')\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"Apply Lifecycle DoFn\" >> beam.ParDo(LifecycleDoFn())\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3b32a9",
   "metadata": {},
   "source": [
    "### **DoFn: Stateful Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda7b7e",
   "metadata": {},
   "source": [
    "- Allows operations that depend on previous or cumulative data.\n",
    "- Reduces the need to rely on external systems, such as databases or caching systems.\n",
    "- Efficient, as the state is stored locally for each key and window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32adfaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.transforms.userstate import CombiningValueStateSpec\n",
    "\n",
    "\"\"\"\n",
    "Exercise: The state is persisted across multiple calls to process()\n",
    "\"\"\"\n",
    "\n",
    "class StatefulDoFn(beam.DoFn):\n",
    "    \n",
    "    # Define a state spec to maintain counts\n",
    "    state_spec = CombiningValueStateSpec(\"count\", sum)\n",
    "\n",
    "    def process(self, element, state=beam.DoFn.StateParam(state_spec)):\n",
    "        k,v = element\n",
    "        state.add(1)\n",
    "        yield f\"Key: {k}, Count: {state.read()}\"\n",
    "    \n",
    "# Interactive pipeline\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./00_DocAux/input_text.txt')\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"Map\" >> beam.Map(lambda x: (x.lower(),1))\n",
    "            | \"Count Events Per Key\" >> beam.ParDo(StatefulDoFn())\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df0cb7",
   "metadata": {},
   "source": [
    "### Beam ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b3c094",
   "metadata": {},
   "source": [
    "- The **RunInference** transform enables the execution of machine learning inference directly within an Apache Beam pipeline by applying a preconfigured model to incoming data elements\n",
    "\n",
    "- **MLTransform** is used within a pipeline to prepare data for machine learning model training. It encapsulates multiple data preprocessing operations into a single abstraction, enabling the application of diverse transformation workflows through a unified interfac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd8ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.6.0 --quiet\n",
    "!pip install tensorflow==2.18.0 --quiet\n",
    "!pip install transformers==4.44.2 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f277aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from typing import Iterable\n",
    "from typing import Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForMaskedLM\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.ml.inference.base import KeyedModelHandler\n",
    "from apache_beam.ml.inference.base import PredictionResult\n",
    "from apache_beam.ml.inference.base import RunInference\n",
    "from apache_beam.ml.inference.huggingface_inference import HuggingFacePipelineModelHandler\n",
    "from apache_beam.ml.inference.huggingface_inference import HuggingFaceModelHandlerKeyedTensor\n",
    "from apache_beam.ml.inference.huggingface_inference import HuggingFaceModelHandlerTensor\n",
    "from apache_beam.ml.inference.huggingface_inference import PipelineTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389cb5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_handler = HuggingFacePipelineModelHandler(\n",
    "    task=PipelineTask.Translation_XX_to_YY,\n",
    "    model = \"google/flan-t5-small\",\n",
    "    load_pipeline_args={'framework': 'pt', \"device\": -1},\n",
    "    inference_args={'max_new_tokens': 200}\n",
    ")\n",
    "\n",
    "class FormatOutput(beam.DoFn):\n",
    "    def process(self, element: PredictionResult[Dict]) -> Iterable[Tuple[str, str]]:\n",
    "        \n",
    "        translated_text = element.inference[0]['translation_text']\n",
    "\n",
    "        logging.info(f\"Translated Text: {translated_text}\")\n",
    "        \n",
    "        yield translated_text\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./00_DocAux/input_text.txt')\n",
    "            | \"RunInference\" >> RunInference(model_handler)\n",
    "            | \"Print\" >> beam.ParDo(FormatOutput())\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d6d503",
   "metadata": {},
   "source": [
    "### **Streaming** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2afee27",
   "metadata": {},
   "source": [
    "#### GCP Setup\n",
    "\n",
    "- PubSub Topics\n",
    "\n",
    "```\n",
    "gcloud pubsub topics create <TOPIC_NAME>\n",
    "```\n",
    "\n",
    "- PubSub Subscriptions\n",
    "\n",
    "```\n",
    "gcloud pubsub subscriptions create <SUBSCRIPTION_NAME> --topic=<TOPIC_NAME>\n",
    "```\n",
    "\n",
    "- Google Cloud Storage Bucket  \n",
    "\n",
    "```\n",
    "gcloud storage mb gs://<BUCKET_NAME> --location=<REGION_ID>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da1a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "project_id = \"\"\n",
    "subscription_name = \"\"\n",
    "bq_dataset = \"\"\n",
    "bq_table = \"\"\n",
    "bucket_name = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbae5c76",
   "metadata": {},
   "source": [
    "**Window in Apache Beam**\n",
    "  \n",
    "A window in Apache Beam defines a time frame for **organizing and grouping data elements** during processing, enabling time-based operations.\n",
    "\n",
    "---\n",
    "\n",
    "**Types of Windows:**\n",
    "\n",
    "- **Fixed Window:** \n",
    "\n",
    "  Divides elements into fixed time intervals, segmenting the PCollection into evenly spaced, time-based windows.\n",
    "\n",
    "- **Sliding Window:**  \n",
    "\n",
    "  Creates overlapping windows with a specified size and stride, enabling continuous analysis of data over time.\n",
    "\n",
    "- **Session Window:**  \n",
    "\n",
    "  Groups elements based on **contiguous temporal activity**, where windows are dynamically defined by an **inactivity gap** between events, making it ideal for capturing logical sessions in streaming data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911fece6",
   "metadata": {},
   "source": [
    "**A. FixedWindows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3af993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.transforms.window import FixedWindows\n",
    "\n",
    "class getWindowDoFn(beam.DoFn):\n",
    "    def process(self, element, window=beam.DoFn.WindowParam):\n",
    "        yield (str(window), element)\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "    events = [\n",
    "        beam.window.TimestampedValue(\"event1\", 60),   \n",
    "        beam.window.TimestampedValue(\"event2\", 180),  \n",
    "        beam.window.TimestampedValue(\"event3\", 360), \n",
    "    ]\n",
    "\n",
    "    (\n",
    "        p\n",
    "        | \"Create PCollection\" >> beam.Create(events)\n",
    "        | \"Sliding Window\" >> beam.WindowInto(FixedWindows(size=300))\n",
    "        | \"k,v\" >> beam.Map(lambda x: (\"key\", x))\n",
    "        | \"Combine\" >> beam.GroupByKey()\n",
    "        | \"Print\" >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2087a3ad",
   "metadata": {},
   "source": [
    "**B. SlidingWindows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed0785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.transforms.window import SlidingWindows\n",
    "\n",
    "class getWindowDoFn(beam.DoFn):\n",
    "    def process(self, element, window=beam.DoFn.WindowParam):\n",
    "        if window.start > 0:\n",
    "            yield (str(window), element)\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "    events = [\n",
    "        beam.window.TimestampedValue(\"event1\", 1),   \n",
    "        beam.window.TimestampedValue(\"event2\", 7),  \n",
    "        beam.window.TimestampedValue(\"event3\", 9), \n",
    "    ]\n",
    "\n",
    "    (\n",
    "        p\n",
    "        | \"Create PCollection\" >> beam.Create(events)\n",
    "        | \"Sliding Window\" >> beam.WindowInto(SlidingWindows(size=10, period=2))\n",
    "        | \"k,v\" >> beam.ParDo(getWindowDoFn())\n",
    "        | \"Combine\" >> beam.GroupByKey()\n",
    "        | \"Print\" >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be21097f",
   "metadata": {},
   "source": [
    "**C. SessionWindows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf567b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.transforms.window import Sessions\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "    events = [\n",
    "        beam.window.TimestampedValue(\"event1\", 0),   \n",
    "        beam.window.TimestampedValue(\"event2\", 5),  \n",
    "        beam.window.TimestampedValue(\"event3\", 25),\n",
    "        beam.window.TimestampedValue(\"event4\", 27), \n",
    "    ]\n",
    "\n",
    "    (\n",
    "        p\n",
    "        | \"Create PCollection\" >> beam.Create(events)\n",
    "        | \"Sliding Window\" >> beam.WindowInto(Sessions(gap_size=7))\n",
    "        | \"k,v\" >> beam.Map(lambda x: (\"key\", x))\n",
    "        | \"Combine\" >> beam.GroupByKey()\n",
    "        | \"Print\" >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874fa2e3",
   "metadata": {},
   "source": [
    "**D. Triggers, Watermark & Allowed Lateness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3736151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.testing.test_stream import TestStream\n",
    "from apache_beam.transforms.window import FixedWindows\n",
    "from apache_beam.transforms.trigger import AfterWatermark, AfterProcessingTime\n",
    "from apache_beam.transforms.trigger import AccumulationMode\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "opts = PipelineOptions(flags=[\"--allow_unsafe_triggers\"],streaming=True, runner=\"DirectRunner\")\n",
    "\n",
    "with beam.Pipeline(options=opts) as p:\n",
    "    events = (\n",
    "        TestStream()\n",
    "        .add_elements([\n",
    "            beam.window.TimestampedValue(\"event1\", 0),\n",
    "            beam.window.TimestampedValue(\"event2\", 2),\n",
    "        ])\n",
    "        .advance_processing_time(5)\n",
    "        .add_elements([\n",
    "            beam.window.TimestampedValue(\"event3\", 4),\n",
    "        ])\n",
    "        .advance_watermark_to(10)\n",
    "        .add_elements([\n",
    "            beam.window.TimestampedValue(\"late_event\", 1),\n",
    "        ])\n",
    "        .advance_watermark_to_infinity()\n",
    "    )\n",
    "\n",
    "    (\n",
    "        p\n",
    "        | \"Stream\" >> events\n",
    "        | \"Window\" >> beam.WindowInto(\n",
    "            FixedWindows(5),\n",
    "            trigger=AfterWatermark(early=AfterProcessingTime(3)),\n",
    "            allowed_lateness=10,\n",
    "            accumulation_mode=AccumulationMode.ACCUMULATING\n",
    "        )\n",
    "        | \"k,v\" >> beam.Map(lambda x: (\"key\", x))\n",
    "        | \"Group\" >> beam.GroupByKey()\n",
    "        | \"Print\" >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8161b20",
   "metadata": {},
   "source": [
    "### Apache Beam on Google Cloud: Dataflow Pipeline from Pub/Sub to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b5418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import apache_beam as beam\n",
    "import logging\n",
    "import json\n",
    "import sys\n",
    "\n",
    "sys.argv = ['']\n",
    "\n",
    "def decode_message(msg):\n",
    "\n",
    "    output = msg.decode('utf-8')\n",
    "    logging.info(\"New PubSub Message: %s\", output)\n",
    "\n",
    "    return json.loads(output)\n",
    "\n",
    "def run():\n",
    "    with beam.Pipeline(options=PipelineOptions(\n",
    "        streaming=True,\n",
    "        job_name = \"edem-pubsub-to-bigquery\",\n",
    "        project=project_id,\n",
    "        runner=\"DataflowRunner\",\n",
    "        temp_location=f\"gs://{bucket_name}/tmp\",\n",
    "        staging_location=f\"gs://{bucket_name}/staging\",\n",
    "        region=\"europe-southwest1\"\n",
    "    )) as p:\n",
    "        \n",
    "        (\n",
    "            p \n",
    "            | \"ReadFromPubSub\" >> beam.io.ReadFromPubSub(subscription=f'projects/{project_id}/subscriptions/{subscription_name}')\n",
    "            | \"decode msg\" >> beam.Map(decode_message)\n",
    "            | \"Write to BigQuery\" >> beam.io.WriteToBigQuery(\n",
    "                table = f\"{project_id}:{bq_dataset}.{bq_table}\", # Required Format: PROJECT_ID:DATASET.TABLE\n",
    "                schema='name:STRING', # Required Format: field_name:TYPE\n",
    "                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Run Process\n",
    "logging.info(\"The process started\")\n",
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
